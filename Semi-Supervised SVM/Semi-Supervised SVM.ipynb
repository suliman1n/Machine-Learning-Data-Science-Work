{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec094afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import array as arr\n",
    "import math\n",
    "import copy as cp\n",
    "import logging\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import operator\n",
    "from time import time\n",
    "import sys\n",
    "from scipy import optimize\n",
    "import scipy.sparse.csc as csc\n",
    "from scipy import sparse\n",
    "import scipy\n",
    "import warnings\n",
    "warnings.simplefilter('error')\n",
    "\n",
    "__author__ =  'Fabian Gieseke, Antti Airola, Tapio Pahikkala, Oliver Kramer'\n",
    "__version__=  '0.1'\n",
    "\n",
    "class QN_S3VM:\n",
    "    \"\"\"\n",
    "    L-BFGS optimizer for semi-supervised support vector machines (S3VM).\n",
    "    \"\"\"\n",
    "    def __init__(self, X_l, L_l, X_u, random_generator = None, ** kw):\n",
    "        \"\"\"\n",
    "        Initializes the model. Detects automatically if dense or sparse data is provided.\n",
    "        Keyword arguments:\n",
    "        X_l -- patterns of labeled part of the data\n",
    "        L_l -- labels of labeled part of the data\n",
    "        X_u -- patterns of unlabeled part of the data\n",
    "        random_generator -- particular instance of a random_generator (default None)\n",
    "        kw -- additional parameters for the optimizer\n",
    "        lam -- regularization parameter lambda (default 1, must be a float > 0)\n",
    "        lamU -- cost parameter that determines influence of unlabeled patterns (default 1, must be float > 0)\n",
    "        sigma -- kernel width for RBF kernel (default 1.0, must be a float > 0)\n",
    "        kernel_type -- \"Linear\" or \"RBF\" (default \"Linear\")\n",
    "        numR -- implementation of subset of regressors. If None is provided, all patterns are used\n",
    "                (no approximation). Must fulfill 0 <= numR <= len(X_l) + len(X_u) (default None)\n",
    "        estimate_r -- desired ratio for positive and negative assigments for \n",
    "                      unlabeled patterns (-1.0 <= estimate_r <= 1.0). If estimate_r=None, \n",
    "                      then L_l is used to estimate this ratio (in case len(L_l) >= \n",
    "                      minimum_labeled_patterns_for_estimate_r. Otherwise use estimate_r = 0.0\n",
    "                      (default None)\n",
    "        minimum_labeled_patterns_for_estimate_r -- see above (default 0)\n",
    "        BFGS_m -- BFGS parameter (default 50)\n",
    "        BFGS_maxfun -- BFGS parameter, maximum number of function calls (default 500)\n",
    "        BFGS_factr -- BFGS parameter (default 1E12)\n",
    "        BFGS_pgtol -- BFGS parameter (default 1.0000000000000001e-05)\n",
    "        \"\"\"\n",
    "        self.__model = None\n",
    "        # Initiate model for sparse data\n",
    "        if isinstance(X_l, csc.csc_matrix):\n",
    "            self.__data_type = \"sparse\"\n",
    "            self.__model = QN_S3VM_Sparse(X_l, L_l, X_u, random_generator, ** kw)\n",
    "        # Initiate model for dense data\n",
    "        elif (isinstance(X_l[0], list)) or (isinstance(X_l[0], np.ndarray)):\n",
    "            self.__data_type = \"dense\"\n",
    "            self.__model = QN_S3VM_Dense(X_l, L_l, X_u, random_generator, ** kw)\n",
    "        # Data format unknown\n",
    "        if self.__model == None:\n",
    "            logging.info(\"Data format for patterns is unknown.\")\n",
    "            sys.exit(0)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Training phase.\n",
    "        Returns:\n",
    "        The computed partition for the unlabeled patterns.\n",
    "        \"\"\"\n",
    "        return self.__model.train()\n",
    "\n",
    "    def getPredictions(self, X, real_valued=False):\n",
    "        \"\"\"\n",
    "        Computes the predicted labels for a given set of patterns\n",
    "        Keyword arguments:\n",
    "        X -- The set of patterns \n",
    "        real_valued -- If True, then the real prediction values are returned\n",
    "        Returns:\n",
    "        The predictions for the list X of patterns.\n",
    "        \"\"\"\n",
    "        return self.__model.getPredictions(X, real_valued=False)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts a label (-1 or +1) for the pattern\n",
    "        Keyword arguments:\n",
    "        x -- The pattern \n",
    "        Returns:\n",
    "        The prediction for x.\n",
    "        \"\"\"\n",
    "        return self.__model.predict(x)\n",
    "\n",
    "    def predictValue(self, x):\n",
    "        \"\"\"\n",
    "        Computes f(x) for a given pattern (see Representer Theorem)\n",
    "    \n",
    "        Keyword arguments:\n",
    "        x -- The pattern \n",
    "        Returns:\n",
    "        The (real) prediction value for x.\n",
    "        \"\"\"\n",
    "        return self.__model.predictValue(x)\n",
    "\n",
    "    def getNeededFunctionCalls(self):\n",
    "        \"\"\"\n",
    "        Returns the number of function calls needed during \n",
    "        the optimization process.\n",
    "        \"\"\"\n",
    "        return self.__model.getNeededFunctionCalls()\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "class QN_S3VM_Dense:\n",
    "\n",
    "    \"\"\"\n",
    "    BFGS optimizer for semi-supervised support vector machines (S3VM).\n",
    "    Dense Data\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "    'lam': 1,\n",
    "    'lamU':1,\n",
    "    'sigma': 1,\n",
    "    'kernel_type': \"Linear\",\n",
    "    'numR':None,\n",
    "    'estimate_r':None,\n",
    "    'minimum_labeled_patterns_for_estimate_r':0,\n",
    "    'BFGS_m':50,\n",
    "    'BFGS_maxfun':500,\n",
    "    'BFGS_factr':1E12,\n",
    "    'BFGS_pgtol':1.0000000000000001e-05,\n",
    "    'BFGS_verbose':-1,\n",
    "    'surrogate_s':3.0,\n",
    "    'surrogate_gamma':20.0,\n",
    "    'breakpoint_for_exp':500\n",
    "    }\n",
    "\n",
    "    def __init__(self, X_l, L_l, X_u, random_generator, ** kw):\n",
    "        \"\"\"\n",
    "        Intializes the S3VM optimizer.\n",
    "        \"\"\"\n",
    "        self.__random_generator = random_generator\n",
    "        self.__X_l, self.__X_u, self.__L_l = X_l, X_u, L_l\n",
    "        assert len(X_l) == len(L_l)\n",
    "        self.__X = cp.deepcopy(self.__X_l)\n",
    "        self.__X.extend(cp.deepcopy(self.__X_u))\n",
    "        self.__size_l, self.__size_u, self.__size_n = len(X_l), len(X_u), len(X_l) + len(X_u)\n",
    "        self.__matrices_initialized = False\n",
    "        self.__setParameters( ** kw)\n",
    "        self.__kw = kw\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Training phase.\n",
    "        Returns:\n",
    "        The computed partition for the unlabeled patterns.\n",
    "        \"\"\"\n",
    "        indi_opt = self.__optimize()\n",
    "        self.__recomputeModel(indi_opt)\n",
    "        predictions = self.__getTrainingPredictions(self.__X)\n",
    "        return predictions\n",
    "\n",
    "    def getPredictions(self, X, real_valued=False):\n",
    "        \"\"\"\n",
    "        Computes the predicted labels for a given set of patterns\n",
    "        Keyword arguments:\n",
    "        X -- The set of patterns \n",
    "        real_valued -- If True, then the real prediction values are returned\n",
    "        Returns:\n",
    "        The predictions for the list X of patterns.\n",
    "        \"\"\"\n",
    "        KNR = self.__kernel.computeKernelMatrix(X, self.__Xreg)\n",
    "        KNU_bar = self.__kernel.computeKernelMatrix(X, self.__X_u_subset, symmetric=False)\n",
    "        KNU_bar_horizontal_sum = (1.0 / len(self.__X_u_subset)) * KNU_bar.sum(axis=1)\n",
    "        KNR = KNR - KNU_bar_horizontal_sum - self.__KU_barR_vertical_sum + self.__KU_barU_bar_sum\n",
    "        preds = KNR * self.__c[0:self.__dim-1,:] + self.__c[self.__dim-1,:]\n",
    "        print(type(preds))\n",
    "        if real_valued == True:\n",
    "            return preds.flatten(1).tolist()[0]\n",
    "        else:\n",
    "            return np.sign(np.sign(preds)+0.1).flatten().tolist()[0]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts a label for the pattern\n",
    "        Keyword arguments:\n",
    "        x -- The pattern \n",
    "        Returns:\n",
    "        The prediction for x.\n",
    "        \"\"\"\n",
    "        return self.getPredictions([x], real_valued=False)[0]\n",
    "        \n",
    "    def predictValue(self, x):\n",
    "        \"\"\"\n",
    "        Computes f(x) for a given pattern (see Representer Theorem)\n",
    "    \n",
    "        Keyword arguments:\n",
    "        x -- The pattern \n",
    "        Returns:\n",
    "        The (real) prediction value for x.\n",
    "        \"\"\"\n",
    "        return self.getPredictions([x], real_valued=True)[0]\n",
    "\n",
    "    def getNeededFunctionCalls(self):\n",
    "        \"\"\"\n",
    "        Returns the number of function calls needed during \n",
    "        the optimization process.\n",
    "        \"\"\"\n",
    "        return self.__needed_function_calls\n",
    "\n",
    "    def __setParameters(self,  ** kw):\n",
    "        for attr, val in kw.items():\n",
    "            self.parameters[attr] = val\n",
    "        self.__lam = float(self.parameters['lam'])\n",
    "        assert self.__lam > 0\n",
    "        self.__lamU = float(self.parameters['lamU'])\n",
    "        assert self.__lamU > 0\n",
    "        self.__lam_Uvec = [float(self.__lamU)*i for i in [0,0.000001,0.0001,0.01,0.1,0.5,1]]\n",
    "        self.__sigma = float(self.parameters['sigma'])\n",
    "        assert self.__sigma > 0\n",
    "        self.__kernel_type = str(self.parameters['kernel_type'])\n",
    "        if self.parameters['numR'] != None:\n",
    "            self.__numR = int(self.parameters['numR'])\n",
    "            assert (self.__numR <= len(self.__X)) and (self.__numR > 0)\n",
    "        else:\n",
    "            self.__numR = len(self.__X)\n",
    "        self.__regressors_indices = sorted(self.__random_generator.sample( range(0,len(self.__X)), self.__numR ))\n",
    "        self.__dim = self.__numR + 1 # add bias term b\n",
    "        self.__minimum_labeled_patterns_for_estimate_r = float(self.parameters['minimum_labeled_patterns_for_estimate_r'])\n",
    "        # If reliable estimate is available or can be estimated, use it, otherwise\n",
    "        # assume classes to be balanced (i.e., estimate_r=0.0)\n",
    "        if self.parameters['estimate_r'] != None:\n",
    "            self.__estimate_r = float(self.parameters['estimate_r'])\n",
    "        elif len(self.__L_l) >= self.__minimum_labeled_patterns_for_estimate_r:\n",
    "            self.__estimate_r = (1.0 / len(self.__L_l)) * np.sum(self.__L_l)\n",
    "        else:\n",
    "            self.__estimate_r = 0.0\n",
    "        self.__BFGS_m = int(self.parameters['BFGS_m'])\n",
    "        self.__BFGS_maxfun = int(self.parameters['BFGS_maxfun'])\n",
    "        self.__BFGS_factr = float(self.parameters['BFGS_factr'])\n",
    "        # This is a hack for 64 bit systems (Linux). The machine precision \n",
    "        # is different for the BFGS optimizer (Fortran code) and we fix this by:\n",
    "        is_64bits = sys.maxsize > 2**32\n",
    "        if is_64bits:\n",
    "            logging.debug(\"64-bit system detected, modifying BFGS_factr!\")\n",
    "            self.__BFGS_factr = 0.000488288*self.__BFGS_factr\n",
    "        self.__BFGS_pgtol = float(self.parameters['BFGS_pgtol'])\n",
    "        self.__BFGS_verbose = int(self.parameters['BFGS_verbose'])\n",
    "        self.__surrogate_gamma = float(self.parameters['surrogate_gamma'])\n",
    "        self.__s = float(self.parameters['surrogate_s'])\n",
    "        self.__breakpoint_for_exp = float(self.parameters['breakpoint_for_exp'])\n",
    "        self.__b = self.__estimate_r\n",
    "        # size of unlabeled patterns to estimate mean (used for balancing constraint)\n",
    "        self.__max_unlabeled_subset_size = 1000\n",
    "\n",
    "\n",
    "    def __optimize(self):\n",
    "        logging.debug(\"Starting optimization with BFGS ...\")\n",
    "        self.__needed_function_calls = 0\n",
    "        self.__initializeMatrices()\n",
    "        # starting point\n",
    "        c_current = zeros(self.__dim, float64)\n",
    "        c_current[self.__dim-1] = self.__b\n",
    "        # Annealing sequence.\n",
    "        for i in range(len(self.__lam_Uvec)):\n",
    "            self.__lamU = self.__lam_Uvec[i]\n",
    "            # crop one dimension (in case the offset b is fixed)\n",
    "            c_current = c_current[:self.__dim-1]\n",
    "            c_current = self.__localSearch(c_current)\n",
    "            # reappend it if needed\n",
    "            c_current = np.append(c_current, self.__b)\n",
    "        f_opt = self.__getFitness(c_current)\n",
    "        return c_current, f_opt\n",
    "\n",
    "    def __localSearch(self, start):\n",
    "        c_opt, f_opt, d = optimize.fmin_l_bfgs_b(self.__getFitness, start, m=self.__BFGS_m, \\\n",
    "                            fprime=self.__getFitness_Prime, maxfun=self.__BFGS_maxfun, factr=self.__BFGS_factr,\\\n",
    "                            pgtol=self.__BFGS_pgtol, iprint=self.__BFGS_verbose)\n",
    "        self.__needed_function_calls += int(d['funcalls'])\n",
    "        return c_opt\n",
    "\n",
    "    def __initializeMatrices(self):\n",
    "        if self.__matrices_initialized == False:\n",
    "            logging.debug(\"Initializing matrices...\")\n",
    "            # Initialize labels\n",
    "            x = arr.array('i')\n",
    "            for l in self.__L_l:\n",
    "                x.append(l)\n",
    "            self.__YL = mat(x, dtype=np.float64)\n",
    "            self.__YL = self.__YL.transpose()\n",
    "            # Initialize kernel matrices\n",
    "            if (self.__kernel_type == \"Linear\"):\n",
    "                self.__kernel = LinearKernel()\n",
    "            elif (self.__kernel_type == \"RBF\"):\n",
    "                self.__kernel = RBFKernel(self.__sigma)\n",
    "            self.__Xreg = (mat(self.__X)[self.__regressors_indices,:].tolist())\n",
    "            self.__KLR = self.__kernel.computeKernelMatrix(self.__X_l,self.__Xreg, symmetric=False)\n",
    "            self.__KUR = self.__kernel.computeKernelMatrix(self.__X_u,self.__Xreg, symmetric=False)\n",
    "            self.__KNR = cp.deepcopy(bmat([[self.__KLR], [self.__KUR]]))\n",
    "            self.__KRR = self.__KNR[self.__regressors_indices,:]\n",
    "            # Center patterns in feature space (with respect to approximated mean of unlabeled patterns in the feature space)\n",
    "            subset_unlabled_indices = sorted(self.__random_generator.sample( range(0,len(self.__X_u)), min(self.__max_unlabeled_subset_size, len(self.__X_u)) ))\n",
    "            self.__X_u_subset = (mat(self.__X_u)[subset_unlabled_indices,:].tolist())\n",
    "            self.__KNU_bar = self.__kernel.computeKernelMatrix(self.__X, self.__X_u_subset, symmetric=False)\n",
    "            self.__KNU_bar_horizontal_sum = (1.0 / len(self.__X_u_subset)) * self.__KNU_bar.sum(axis=1)\n",
    "            self.__KU_barR = self.__kernel.computeKernelMatrix(self.__X_u_subset, self.__Xreg, symmetric=False)\n",
    "            self.__KU_barR_vertical_sum = (1.0 / len(self.__X_u_subset)) * self.__KU_barR.sum(axis=0)\n",
    "            self.__KU_barU_bar = self.__kernel.computeKernelMatrix(self.__X_u_subset, self.__X_u_subset, symmetric=False)\n",
    "            self.__KU_barU_bar_sum = (1.0 / (len(self.__X_u_subset)))**2 * self.__KU_barU_bar.sum()\n",
    "            self.__KNR = self.__KNR - self.__KNU_bar_horizontal_sum - self.__KU_barR_vertical_sum + self.__KU_barU_bar_sum\n",
    "            self.__KRR = self.__KNR[self.__regressors_indices,:]\n",
    "            self.__KLR = self.__KNR[range(0,len(self.__X_l)),:]\n",
    "            self.__KUR = self.__KNR[range(len(self.__X_l),len(self.__X)),:]\n",
    "            self.__matrices_initialized = True\n",
    "\n",
    "    def __getFitness(self,c):\n",
    "        # Check whether the function is called from the bfgs solver \n",
    "        # (that does not optimize the offset b) or not\n",
    "        if len(c) == self.__dim - 1:\n",
    "            c = np.append(c, self.__b)\n",
    "        c = mat(c)\n",
    "        b = c[:,self.__dim-1].T\n",
    "        c_new = c[:,0:self.__dim-1].T\n",
    "        preds_labeled = self.__surrogate_gamma*(1.0 - multiply(self.__YL, self.__KLR * c_new + b))\n",
    "        preds_unlabeled = self.__KUR * c_new + b\n",
    "        # This vector has a \"one\" for each \"numerically instable\" entry; \"zeros\" for \"good ones\". \n",
    "        preds_labeled_conflict_indicator = np.sign(np.sign(preds_labeled/self.__breakpoint_for_exp - 1.0) + 1.0)\n",
    "        # This vector has a one for each good entry and zero otherwise\n",
    "        preds_labeled_good_indicator = (-1)*(preds_labeled_conflict_indicator - 1.0)\n",
    "        preds_labeled_for_conflicts = multiply(preds_labeled_conflict_indicator,preds_labeled) \n",
    "        preds_labeled = multiply(preds_labeled,preds_labeled_good_indicator)\n",
    "        # Compute values for good entries\n",
    "        preds_labeled_log_exp = np.log(1.0 + np.exp(preds_labeled))\n",
    "        # Compute values for instable entries\n",
    "        preds_labeled_log_exp = multiply(preds_labeled_good_indicator, preds_labeled_log_exp)\n",
    "        # Replace critical values with values \n",
    "        preds_labeled_final = preds_labeled_log_exp + preds_labeled_for_conflicts\n",
    "        term1 = (1.0/(self.__surrogate_gamma*self.__size_l)) * np.sum(preds_labeled_final)\n",
    "        preds_unlabeled_squared = multiply(preds_unlabeled,preds_unlabeled)\n",
    "        term2 = (float(self.__lamU)/float(self.__size_u))*np.sum(np.exp(-self.__s * preds_unlabeled_squared))\n",
    "        term3 = self.__lam * (c_new.T * self.__KRR * c_new)\n",
    "        return (term1 + term2 + term3)[0,0]\n",
    "\n",
    "    def __getFitness_Prime(self,c):\n",
    "        # Check whether the function is called from the bfgs solver \n",
    "        # (that does not optimize the offset b) or not\n",
    "        if len(c) == self.__dim - 1:\n",
    "            c = np.append(c, self.__b)\n",
    "        c = mat(c)\n",
    "        b = c[:,self.__dim-1].T\n",
    "        c_new = c[:,0:self.__dim-1].T\n",
    "        preds_labeled = self.__surrogate_gamma * (1.0 - multiply(self.__YL, self.__KLR * c_new + b))\n",
    "        preds_unlabeled = (self.__KUR * c_new + b)\n",
    "        # This vector has a \"one\" for each \"numerically instable\" entry; \"zeros\" for \"good ones\". \n",
    "        preds_labeled_conflict_indicator = np.sign(np.sign(preds_labeled/self.__breakpoint_for_exp - 1.0) + 1.0)\n",
    "        # This vector has a one for each good entry and zero otherwise\n",
    "        preds_labeled_good_indicator = (-1)*(preds_labeled_conflict_indicator - 1.0)\n",
    "        preds_labeled = multiply(preds_labeled,preds_labeled_good_indicator)\n",
    "        preds_labeled_exp = np.exp(preds_labeled)\n",
    "        term1 = multiply(preds_labeled_exp, 1.0/(1.0 + preds_labeled_exp))\n",
    "        term1 = multiply(preds_labeled_good_indicator, term1)\n",
    "        # Replace critical values with \"1.0\"\n",
    "        term1 = term1 + preds_labeled_conflict_indicator\n",
    "        term1 = multiply(self.__YL, term1)\n",
    "        preds_unlabeled_squared_exp_f = multiply(preds_unlabeled,preds_unlabeled)\n",
    "        preds_unlabeled_squared_exp_f = np.exp(-self.__s * preds_unlabeled_squared_exp_f)\n",
    "        preds_unlabeled_squared_exp_f = multiply(preds_unlabeled_squared_exp_f, preds_unlabeled)\n",
    "        term1 = (-1.0/self.__size_l) * (term1.T * self.__KLR).T\n",
    "        term2 = ((-2.0 * self.__s * self.__lamU)/float(self.__size_u)) * (preds_unlabeled_squared_exp_f.T * self.__KUR).T\n",
    "        term3 = 2*self.__lam*(self.__KRR * c_new)\n",
    "        return array((term1 + term2 + term3).T)[0]\n",
    "\n",
    "    def __recomputeModel(self, indi):\n",
    "        self.__c = mat(indi[0]).T\n",
    "\n",
    "    def __getTrainingPredictions(self, X, real_valued=False):\n",
    "        preds = self.__KNR * self.__c[0:self.__dim-1,:] + self.__c[self.__dim-1,:]\n",
    "        if real_valued == True:\n",
    "            return preds.flatten(1).tolist()[0]\n",
    "        else:\n",
    "            return np.sign(np.sign(preds)+0.1).flatten().tolist()[0]\n",
    "\n",
    "    def __check_matrix(self, M):\n",
    "        smallesteval = scipy.linalg.eigvalsh(M, eigvals=(0,0))[0]\n",
    "        if smallesteval < 0.0:\n",
    "            shift = abs(smallesteval) + 0.0000001 \n",
    "            M = M + shift\n",
    "        return M\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "class QN_S3VM_Sparse:\n",
    "    \"\"\"\n",
    "    BFGS optimizer for semi-supervised support vector machines (S3VM).\n",
    "    Sparse Data\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "    'lam': 1,\n",
    "    'lamU':1,\n",
    "    'estimate_r':None,\n",
    "    'minimum_labeled_patterns_for_estimate_r':0,\n",
    "    'BFGS_m':50,\n",
    "    'BFGS_maxfun':500,\n",
    "    'BFGS_factr':1E12,\n",
    "    'BFGS_pgtol':1.0000000000000001e-05,\n",
    "    'BFGS_verbose':-1,\n",
    "    'surrogate_s':3.0,\n",
    "    'surrogate_gamma':20.0,\n",
    "    'breakpoint_for_exp':500\n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(self, X_l, L_l, X_u, random_generator, ** kw):\n",
    "        \"\"\"\n",
    "        Intializes the S3VM optimizer.\n",
    "        \"\"\"\n",
    "        self.__random_generator = random_generator\n",
    "        # This is a nuisance, but we may need to pad extra dimensions to either X_l or X_u\n",
    "        # in case the highest feature indices appear only in one of the two data matrices\n",
    "        if X_l.shape[1] > X_u.shape[1]:\n",
    "            X_u = sparse.hstack([X_u, sparse.coo_matrix(X_u.shape[0], X_l.shape[1] - X_u.shape[1])])\n",
    "        elif X_l.shape[1] < X_u.shape[1]:\n",
    "            X_l = sparse.hstack([X_l, sparse.coo_matrix(X_l.shape[0], X_u.shape[1] - X_u.shape[1])])\n",
    "        # We vertically stack the data matrices into one big matrix\n",
    "        X = sparse.vstack([X_l, X_u])\n",
    "        self.__size_l, self.__size_u, self.__size_n = X_l.shape[0], X_u.shape[0], X_l.shape[0]+ X_u.shape[0]\n",
    "        x = arr.array('i')\n",
    "        for l in L_l:\n",
    "            x.append(int(l))\n",
    "        self.__YL = mat(x, dtype=np.float64)\n",
    "        self.__YL = self.__YL.transpose()\n",
    "        self.__setParameters( ** kw)\n",
    "        self.__kw = kw\n",
    "        self.X_l = X_l.tocsr()\n",
    "        self.X_u = X_u.tocsr()\n",
    "        self.X = X.tocsr()\n",
    "        # compute mean of unlabeled patterns\n",
    "        self.__mean_u = self.X_u.mean(axis=0)\n",
    "        self.X_u_T = X_u.tocsc().T\n",
    "        self.X_l_T = X_l.tocsc().T\n",
    "        self.X_T = X.tocsc().T\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Training phase.\n",
    "        Returns:\n",
    "        The computed partition for the unlabeled patterns.\n",
    "        \"\"\"\n",
    "        indi_opt = self.__optimize()\n",
    "        self.__recomputeModel(indi_opt)\n",
    "        predictions = self.getPredictions(self.X)\n",
    "        return predictions\n",
    "\n",
    "    def getPredictions(self, X, real_valued=False):\n",
    "        \"\"\"\n",
    "        Computes the predicted labels for a given set of patterns\n",
    "        Keyword arguments:\n",
    "        X -- The set of patterns \n",
    "        real_valued -- If True, then the real prediction values are returned\n",
    "        Returns:\n",
    "        The predictions for the list X of patterns.\n",
    "        \"\"\"\n",
    "        c_new = self.__c[:self.__dim-1]\n",
    "        W = self.X.T*c_new - self.__mean_u.T*np.sum(c_new)\n",
    "        # Again, possibility of dimension mismatch due to use of sparse matrices\n",
    "        if X.shape[1] > W.shape[0]:\n",
    "            X = X[:,range(W.shape[0])]\n",
    "        if X.shape[1] < W.shape[0]:\n",
    "            W = W[range(X.shape[1])]\n",
    "        X = X.tocsc()\n",
    "        preds = X * W + self.__b\n",
    "        print(type(preds))\n",
    "        if real_valued == True:\n",
    "            return preds.flatten().tolist()[0]\n",
    "        else:\n",
    "            return np.sign(np.sign(preds)+0.1).flatten().tolist()[0]\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts a label for the pattern\n",
    "        Keyword arguments:\n",
    "        x -- The pattern \n",
    "        Returns:\n",
    "        The prediction for x.\n",
    "        \"\"\"\n",
    "        return self.getPredictions([x], real_valued=False)[0]\n",
    "        \n",
    "    def predictValue(self, x):\n",
    "        \"\"\"\n",
    "        Computes f(x) for a given pattern (see Representer Theorem)\n",
    "    \n",
    "        Keyword arguments:\n",
    "        x -- The pattern \n",
    "        Returns:\n",
    "        The (real) prediction value for x.\n",
    "        \"\"\"\n",
    "        return self.getPredictions([x], real_valued=True)[0]\n",
    "\n",
    "    def getNeededFunctionCalls(self):\n",
    "        \"\"\"\n",
    "        Returns the number of function calls needed during \n",
    "        the optimization process.\n",
    "        \"\"\"\n",
    "        return self.__needed_function_calls\n",
    "\n",
    "    def __setParameters(self,  ** kw):\n",
    "        for attr, val in kw.items():\n",
    "            self.parameters[attr] = val\n",
    "        self.__lam = float(self.parameters['lam'])\n",
    "        assert self.__lam > 0\n",
    "        self.__lamU = float(self.parameters['lamU'])\n",
    "        assert self.__lamU > 0\n",
    "        self.__lam_Uvec = [float(self.__lamU)*i for i in [0,0.000001,0.0001,0.01,0.1,0.5,1]]\n",
    "        self.__minimum_labeled_patterns_for_estimate_r = float(self.parameters['minimum_labeled_patterns_for_estimate_r'])\n",
    "        # If reliable estimate is available or can be estimated, use it, otherwise\n",
    "        # assume classes to be balanced (i.e., estimate_r=0.0)\n",
    "        if self.parameters['estimate_r'] != None:\n",
    "            self.__estimate_r = float(self.parameters['estimate_r'])\n",
    "        elif self.__YL.shape[0] > self.__minimum_labeled_patterns_for_estimate_r:\n",
    "            self.__estimate_r = (1.0 / self.__YL.shape[0]) * np.sum(self.__YL[0:])\n",
    "        else:\n",
    "            self.__estimate_r = 0.0\n",
    "        self.__dim = self.__size_n + 1 # for offset term b\n",
    "        self.__BFGS_m = int(self.parameters['BFGS_m'])\n",
    "        self.__BFGS_maxfun = int(self.parameters['BFGS_maxfun'])\n",
    "        self.__BFGS_factr = float(self.parameters['BFGS_factr'])\n",
    "        # This is a hack for 64 bit systems (Linux). The machine precision \n",
    "        # is different for the BFGS optimizer (Fortran code) and we fix this by:\n",
    "        is_64bits = sys.maxsize > 2**32\n",
    "        if is_64bits:\n",
    "            logging.debug(\"64-bit system detected, modifying BFGS_factr!\")\n",
    "            self.__BFGS_factr = 0.000488288*self.__BFGS_factr\n",
    "        self.__BFGS_pgtol = float(self.parameters['BFGS_pgtol'])\n",
    "        self.__BFGS_verbose = int(self.parameters['BFGS_verbose'])\n",
    "        self.__surrogate_gamma = float(self.parameters['surrogate_gamma'])\n",
    "        self.__s = float(self.parameters['surrogate_s'])\n",
    "        self.__breakpoint_for_exp = float(self.parameters['breakpoint_for_exp'])\n",
    "        self.__b = self.__estimate_r\n",
    "\n",
    "    def __optimize(self):\n",
    "        logging.debug(\"Starting optimization with BFGS ...\")\n",
    "        self.__needed_function_calls = 0\n",
    "        # starting_point\n",
    "        c_current = zeros(self.__dim, float64)\n",
    "        c_current[self.__dim-1] = self.__b\n",
    "        # Annealing sequence.\n",
    "        for i in range(len(self.__lam_Uvec)):\n",
    "            self.__lamU = self.__lam_Uvec[i]\n",
    "            # crop one dimension (in case the offset b is fixed)\n",
    "            c_current = c_current[:self.__dim-1]\n",
    "            c_current = self.__localSearch(c_current)\n",
    "            # reappend it if needed\n",
    "            c_current = np.append(c_current, self.__b)\n",
    "        f_opt = self.__getFitness(c_current)\n",
    "        return c_current, f_opt\n",
    "\n",
    "    def __localSearch(self, start):\n",
    "        c_opt, f_opt, d = optimize.fmin_l_bfgs_b(self.__getFitness, start, m=self.__BFGS_m, \\\n",
    "                                     fprime=self.__getFitness_Prime, maxfun=self.__BFGS_maxfun,\\\n",
    "                                     factr=self.__BFGS_factr, pgtol=self.__BFGS_pgtol, iprint=self.__BFGS_verbose)\n",
    "        self.__needed_function_calls += int(d['funcalls'])\n",
    "        return c_opt\n",
    "\n",
    "    def __getFitness(self,c):\n",
    "        # check whether the function is called from the bfgs solver \n",
    "        # (that does not optimize the offset b) or not\n",
    "        if len(c) == self.__dim - 1:\n",
    "            c = np.append(c, self.__b)\n",
    "        c = mat(c)\n",
    "        b = c[:,self.__dim-1].T\n",
    "        c_new = c[:,0:self.__dim-1].T\n",
    "        c_new_sum = np.sum(c_new)\n",
    "        XTc = self.X_T*c_new - self.__mean_u.T*c_new_sum\n",
    "        preds_labeled = self.__surrogate_gamma*(1.0 - multiply(self.__YL, (self.X_l*XTc - self.__mean_u*XTc) + b[0,0]))\n",
    "        preds_unlabeled = (self.X_u*XTc - self.__mean_u*XTc)  + b[0,0]\n",
    "        # This vector has a \"one\" for each \"numerically instable\" entry; \"zeros\" for \"good ones\". \n",
    "        preds_labeled_conflict_indicator = np.sign(np.sign(preds_labeled/self.__breakpoint_for_exp - 1.0) + 1.0)\n",
    "        # This vector has a one for each good entry and zero otherwise\n",
    "        preds_labeled_good_indicator = (-1)*(preds_labeled_conflict_indicator - 1.0)\n",
    "        preds_labeled_for_conflicts = multiply(preds_labeled_conflict_indicator,preds_labeled) \n",
    "        preds_labeled = multiply(preds_labeled,preds_labeled_good_indicator)\n",
    "        # Compute values for good entries\n",
    "        preds_labeled_log_exp = np.log(1.0 + np.exp(preds_labeled))\n",
    "        # Compute values for instable entries\n",
    "        preds_labeled_log_exp = multiply(preds_labeled_good_indicator, preds_labeled_log_exp)\n",
    "        # Replace critical values with values \n",
    "        preds_labeled_final = preds_labeled_log_exp + preds_labeled_for_conflicts\n",
    "        term1 = (1.0/(self.__surrogate_gamma*self.__size_l)) * np.sum(preds_labeled_final)\n",
    "        preds_unlabeled_squared = multiply(preds_unlabeled,preds_unlabeled)\n",
    "        term2 = (float(self.__lamU)/float(self.__size_u))*np.sum(np.exp(-self.__s * preds_unlabeled_squared))\n",
    "        term3 = self.__lam * c_new.T * (self.X * XTc - self.__mean_u*XTc)\n",
    "        return (term1 + term2 + term3)[0,0]\n",
    "\n",
    "    def __getFitness_Prime(self,c):\n",
    "        # check whether the function is called from the bfgs solver \n",
    "        # (that does not optimize the offset b) or not\n",
    "        if len(c) == self.__dim - 1:\n",
    "            c = np.append(c, self.__b)\n",
    "        c = mat(c)\n",
    "        b = c[:,self.__dim-1].T\n",
    "        c_new = c[:,0:self.__dim-1].T\n",
    "        c_new_sum = np.sum(c_new)\n",
    "        XTc = self.X_T*c_new - self.__mean_u.T*c_new_sum\n",
    "        preds_labeled = self.__surrogate_gamma*(1.0 - multiply(self.__YL, (self.X_l*XTc -self.__mean_u*XTc) + b[0,0]))\n",
    "        preds_unlabeled = (self.X_u*XTc - self.__mean_u*XTc )+ b[0,0]\n",
    "        preds_labeled_conflict_indicator = np.sign(np.sign(preds_labeled/self.__breakpoint_for_exp - 1.0) + 1.0)\n",
    "        # This vector has a one for each good entry and zero otherwise\n",
    "        preds_labeled_good_indicator = (-1)*(preds_labeled_conflict_indicator - 1.0)\n",
    "        preds_labeled = multiply(preds_labeled,preds_labeled_good_indicator)\n",
    "        preds_labeled_exp = np.exp(preds_labeled)\n",
    "        term1 = multiply(preds_labeled_exp, 1.0/(1.0 + preds_labeled_exp))\n",
    "        term1 = multiply(preds_labeled_good_indicator, term1)\n",
    "        # Replace critical values with \"1.0\"\n",
    "        term1 = term1 + preds_labeled_conflict_indicator\n",
    "        term1 = multiply(self.__YL, term1)\n",
    "        preds_unlabeled_squared_exp_f = multiply(preds_unlabeled,preds_unlabeled)\n",
    "        preds_unlabeled_squared_exp_f = np.exp(-self.__s * preds_unlabeled_squared_exp_f)\n",
    "        preds_unlabeled_squared_exp_f = multiply(preds_unlabeled_squared_exp_f, preds_unlabeled)\n",
    "        term1_sum = np.sum(term1)\n",
    "        tmp = self.X_l_T * term1 - self.__mean_u.T*term1_sum\n",
    "        term1 = (-1.0/self.__size_l) * (self.X * tmp - self.__mean_u*tmp)\n",
    "        preds_unlabeled_squared_exp_f_sum = np.sum(preds_unlabeled_squared_exp_f)\n",
    "        tmp_unlabeled = self.X_u_T * preds_unlabeled_squared_exp_f - self.__mean_u.T * preds_unlabeled_squared_exp_f_sum\n",
    "        term2 = ((-2.0 * self.__s * self.__lamU)/float(self.__size_u)) * (self.X * tmp_unlabeled - self.__mean_u*tmp_unlabeled)\n",
    "        XTc_sum = np.sum(XTc)\n",
    "        term3 = 2*self.__lam*(self.X * XTc - self.__mean_u*XTc)\n",
    "        return array((term1 + term2 + term3).T)[0]\n",
    "\n",
    "    def __recomputeModel(self, indi):\n",
    "        self.__c = mat(indi[0]).T\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "class LinearKernel():\n",
    "    \"\"\"\n",
    "    Linear Kernel\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def computeKernelMatrix(self, data1, data2, symmetric=False):\n",
    "        \"\"\"\n",
    "        Computes the kernel matrix\n",
    "        \"\"\"\n",
    "        logging.debug(\"Starting Linear Kernel Matrix Computation...\")\n",
    "        self._data1 = mat(data1)\n",
    "        self._data2 = mat(data2)\n",
    "        assert self._data1.shape[1] == (self._data2.T).shape[0]\n",
    "        try:\n",
    "            return self._data1 * self._data2.T\n",
    "        except:\n",
    "            logging.error(\"Error while computing kernel matrix: \" + str(e))\n",
    "            sys.exit()\n",
    "        logging.debug(\"Kernel Matrix computed...\")\n",
    "\n",
    "    def getKernelValue(self, xi, xj):\n",
    "        \"\"\"\n",
    "        Returns a single kernel value.\n",
    "        \"\"\"\n",
    "        xi = array(xi)\n",
    "        xj = array(xj)\n",
    "        val = dot(xi, xj)\n",
    "        return val\n",
    "\n",
    "\n",
    "class DictLinearKernel():\n",
    "    \"\"\"\n",
    "    Linear Kernel (for dictionaries)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def computeKernelMatrix(self, data1, data2, symmetric=False):\n",
    "        \"\"\"\n",
    "        Computes the kernel matrix\n",
    "        \"\"\"\n",
    "        logging.debug(\"Starting Linear Kernel Matrix Computation...\")\n",
    "        self._data1 = data1\n",
    "        self._data2 = data2\n",
    "        self._dim1 = len(data1)\n",
    "        self._dim2 = len(data2)\n",
    "        self._symmetric = symmetric\n",
    "        self.__km = None\n",
    "        try:\n",
    "            km = mat(zeros((self._dim1, self._dim2), dtype=float64))\n",
    "            if self._symmetric:\n",
    "                for i in xrange(self._dim1):\n",
    "                    message = 'Kernel Matrix Progress: %dx%d/%dx%d' % (i, self._dim2,self._dim1,self._dim2)\n",
    "                    logging.debug(message)\n",
    "                    for j in xrange(i, self._dim2):\n",
    "                        val = self.getKernelValue(self._data1[i], self._data2[j])\n",
    "                        km[i, j] = val\n",
    "                        km[j, i] = val\n",
    "                return km\n",
    "            else:\n",
    "                for i in xrange(self._dim1):\n",
    "                    message = 'Kernel Matrix Progress: %dx%d/%dx%d' % (i, self._dim2,self._dim1,self._dim2)\n",
    "                    logging.debug(message)\n",
    "                    for j in xrange(0, self._dim2):\n",
    "                        val = self.getKernelValue(self._data1[i], self._data2[j])\n",
    "                        km[i, j] = val\n",
    "                return km\n",
    "            \n",
    "        except:\n",
    "            logging.error(\"Error while computing kernel matrix: \" + str(e))\n",
    "            sys.exit()\n",
    "        logging.debug(\"Kernel Matrix computed...\")\n",
    "\n",
    "    def getKernelValue(self, xi, xj):\n",
    "        \"\"\"\n",
    "        Returns a single kernel value.\n",
    "        \"\"\"\n",
    "        val = 0.\n",
    "        for key in xi:\n",
    "            if key in xj:\n",
    "                val += xi[key]*xj[key]\n",
    "        return val\n",
    "\n",
    "class RBFKernel():\n",
    "    \"\"\"\n",
    "    RBF Kernel\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.__sigma = sigma\n",
    "        self.__sigma_squared_inv = 1.0 / (2* (self.__sigma ** 2) )\n",
    "\n",
    "    def computeKernelMatrix(self, data1, data2, symmetric=False):\n",
    "        \"\"\"\n",
    "        Computes the kernel matrix\n",
    "        \"\"\"\n",
    "        print(\"Starting RBF Kernel Matrix Computation...\")\n",
    "        self._data1 = mat(data1)\n",
    "        self._data2 = mat(data2)\n",
    "        assert self._data1.shape[1] == (self._data2.T).shape[0]\n",
    "        self._dim1 = len(data1)\n",
    "        self._dim2 = len(data2)\n",
    "        self._symmetric = symmetric\n",
    "        print(\"Symmetric: \", symmetric)\n",
    "        self.__km = None\n",
    "        try:\n",
    "            if self._symmetric:\n",
    "                linearkm = self._data1 * self._data2.T\n",
    "                trnorms = mat(np.diag(linearkm)).T\n",
    "                trace_matrix = trnorms * mat(np.ones((1, self._dim1), dtype = float64))\n",
    "                self.__km = trace_matrix + trace_matrix.T\n",
    "                self.__km = self.__km - 2*linearkm\n",
    "                self.__km = - self.__sigma_squared_inv * self.__km\n",
    "                self.__km = np.exp(self.__km)\n",
    "                return self.__km   \n",
    "            else:\n",
    "                m = self._data1.shape[0]\n",
    "                n = self._data2.shape[0]\n",
    "                assert self._data1.shape[1] == self._data2.shape[1]\n",
    "                linkm = mat(self._data1 * self._data2.T)\n",
    "                trnorms1 = []\n",
    "                for i in range(m):\n",
    "                    trnorms1.append((self._data1[i] * self._data1[i].T)[0,0])\n",
    "                trnorms1 = mat(trnorms1).T\n",
    "                trnorms2 = []\n",
    "                for i in range(n):\n",
    "                    trnorms2.append((self._data2[i] * self._data2[i].T)[0,0])\n",
    "                trnorms2 = mat(trnorms2).T\n",
    "                self.__km = trnorms1 * mat(np.ones((n, 1), dtype = float64)).T\n",
    "                self.__km = self.__km + mat(np.ones((m, 1), dtype = float64)) * trnorms2.T\n",
    "                self.__km = self.__km - 2 * linkm\n",
    "                self.__km = - self.__sigma_squared_inv * self.__km\n",
    "                self.__km = np.exp(self.__km)\n",
    "                return self.__km\n",
    "        except:\n",
    "            logging.error(\"Error while computing kernel matrix: \" + str(e))\n",
    "            sys.exit()\n",
    "\n",
    "    def getKernelValue(self, xi, xj):\n",
    "        \"\"\"\n",
    "        Returns a single kernel value.\n",
    "        \"\"\"\n",
    "        xi = array(xi)\n",
    "        xj = array(xj)\n",
    "        diff = xi-xj\n",
    "        val = exp(-self.__sigma_squared_inv * (dot(diff, diff)))\n",
    "        return val\n",
    "\n",
    "class DictRBFKernel():\n",
    "    \"\"\"\n",
    "    RBF Kernel (for dictionaries)\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.__sigma = sigma\n",
    "        self.__sigma_squared_inv = 1.0 / ((self.__sigma ** 2))\n",
    "\n",
    "    def computeKernelMatrix(self, data1, data2, symmetric=False):\n",
    "        \"\"\"\n",
    "        Computes the kernel matrix\n",
    "        \"\"\"\n",
    "        logging.debug(\"Starting RBF Kernel Matrix Computation...\")\n",
    "        self._data1 = data1\n",
    "        self._data2 = data2\n",
    "        self._dim1 = len(data1)\n",
    "        self._dim2 = len(data2)\n",
    "        self._symmetric = symmetric\n",
    "        self.__km = None\n",
    "        try:\n",
    "            km = mat(zeros((self._dim1, self._dim2), dtype=float64))\n",
    "            if self._symmetric:\n",
    "                for i in xrange(self._dim1):\n",
    "                    message = 'Kernel Matrix Progress: %dx%d/%dx%d' % (i, self._dim2,self._dim1,self._dim2)\n",
    "                    logging.debug(message)\n",
    "                    for j in xrange(i, self._dim2):\n",
    "                        val = self.getKernelValue(self._data1[i], self._data2[j])\n",
    "                        km[i, j] = val\n",
    "                        km[j, i] = val\n",
    "                return km\n",
    "            else:\n",
    "                for i in xrange(0, self._dim1):\n",
    "                    message = 'Kernel Matrix Progress: %dx%d/%dx%d' % (i, self._dim2,self._dim1,self._dim2)\n",
    "                    logging.debug(message)\n",
    "                    for j in xrange(0, self._dim2):\n",
    "                        val = self.getKernelValue(self._data1[i], self._data2[j])\n",
    "                        km[i, j] = val\n",
    "                return km\n",
    "        except:\n",
    "            logging.error(\"Error while computing kernel matrix: \" + str(e))\n",
    "            sys.exit()\n",
    "        logging.info(\"Kernel Matrix computed...\")\n",
    "\n",
    "    def getKernelValue(self, xi, xj):\n",
    "        \"\"\"\n",
    "        Returns a single kernel value.\n",
    "        \"\"\"\n",
    "        diff = xi.copy()\n",
    "        for key in xj:\n",
    "            if key in diff:\n",
    "                diff[key]-=xj[key]\n",
    "            else:\n",
    "                diff[key]=-xj[key]\n",
    "        diff = diff.values()\n",
    "        val = exp(-self.__sigma_squared_inv * (dot(diff, diff)))\n",
    "        return val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_train_data=pd.read_csv(r'ssl_train_data.csv')\n",
    "test_data=pd.read_csv(r'test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e358d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=ssl_train_data.loc[:,'feat_0':'feat_9']\n",
    "y_train=ssl_train_data.loc[:,'class']\n",
    "\n",
    "X_test=test_data.loc[:,'feat_0':'feat_9']\n",
    "y_test=test_data.loc[:,'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a437a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(a)\n",
    "model = svm.SVC(kernel='linear',C=1)\n",
    "model.fit(X_train,y_train)\n",
    "print(accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(b)\n",
    "accuracy_scores1=[]\n",
    "for L in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    model = svm.SVC(kernel='linear',C=1)\n",
    "    model.fit(X_train.loc[0:L*2-1,:],y_train.loc[0:L*2-1])\n",
    "    accuracy_scores1.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "    print('for L=',L,'accuracy score=',accuracy_score(y_test, model.predict(X_test)))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "plt.plot([2,4,6,8,10,12,14,16,18,20],accuracy_scores1,label='supervised approach')\n",
    "plt.xlabel('L')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d3c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(c)\n",
    "accuracy_scores2=[]\n",
    "my_random_generator = random.Random()\n",
    "#my_random_generator.seed(0)\n",
    "    \n",
    "y_train_np=np.array(y_train)\n",
    "y_train_np[y_train_np==0] = -1\n",
    "y_test_np=np.array(y_test)\n",
    "y_test_np[y_test_np==0]=-1\n",
    "    \n",
    "for L in np.arange(1,11,1):\n",
    "    model = QN_S3VM(X_train.loc[0:L*2-1,:].values.tolist(), y_train_np[0:L*2].tolist(), X_train.loc[L*2:199,:].values.tolist() ,my_random_generator,kernel_type='Linear', lam=1)\n",
    "    model.train()\n",
    "    y_test_pred = model.getPredictions(X_test.values.tolist())\n",
    "    accuracy_scores2.append(accuracy_score(y_test_np.tolist(),y_test_pred))\n",
    "    print('for L=',L,'accuracy score=',accuracy_score(y_test_np.tolist(),y_test_pred))\n",
    "\n",
    "    \n",
    "    \n",
    "plt.plot(np.arange(2,22,2),accuracy_scores1,label='supervised approach')\n",
    "plt.plot(np.arange(2,22,2),accuracy_scores2,label='semi-supervised approach')\n",
    "plt.xlabel('2L')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
