{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_train=pd.read_csv(r'data_source_train.csv')\n",
    "data_source_test=pd.read_csv(r'data_source_test.csv')\n",
    "\n",
    "\n",
    "data_target_labeled=pd.read_csv(r'data_target_labeled.csv')\n",
    "data_target_unlabeled=pd.read_csv(r'data_target_unlabeled.csv')\n",
    "data_target_test=pd.read_csv(r'data_target_test.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2469d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(a)\n",
    "model= AdaBoostClassifier()\n",
    "model.fit(data_source_train[['feat_0','feat_1']],data_source_train['class'])\n",
    "print(accuracy_score(data_source_test['class'], model.predict(data_source_test[['feat_0','feat_1']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(b)\n",
    "print(accuracy_score(data_target_test['class'], model.predict(data_target_test[['feat_0','feat_1']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ce626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(c)\n",
    "model= AdaBoostClassifier()\n",
    "model.fit(data_target_labeled[['feat_0','feat_1']],data_target_labeled['class'])\n",
    "print(accuracy_score(data_target_test['class'], model.predict(data_target_test[['feat_0','feat_1']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(d)\n",
    "model= AdaBoostClassifier()\n",
    "model.fit(data_source_train[['feat_0','feat_1']].append(data_target_labeled[['feat_0','feat_1']]),data_source_train['class'].append(data_target_labeled['class']))\n",
    "print(accuracy_score(data_target_test['class'], model.predict(data_target_test[['feat_0','feat_1']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18369517",
   "metadata": {},
   "source": [
    "#(e)\n",
    "The classifier performed relatively well when it was trained and tested on the source domain.\n",
    "\n",
    "The classifier performed poorly in the target domain when it was trained only with the limted number of labled data in the target domain, but the performance enhanced when we added the source training points to our training procedure.\n",
    "\n",
    "We notice that the classifier performed equivalently when it was trained using the source training only, and when it was trained using the source training with extra labeled data in the target domanin, and this probably because the number of labeled points in the target domain is very few and it will not make a difference in enhancing the model's performace on the target domanin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19185884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(f)(i)\n",
    "gm = GaussianMixture(n_components=2,random_state=0).fit(data_source_train[['feat_0','feat_1']].append(data_target_unlabeled[['feat_0','feat_1']].append(data_target_labeled[['feat_0','feat_1']])))\n",
    "print(gm.means_)\n",
    "print('----------------')\n",
    "print(gm.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(ii)\n",
    "#source\n",
    "print('for source')\n",
    "gm = GaussianMixture(n_components=1,random_state=0).fit(data_source_train[['feat_0','feat_1']])\n",
    "mu_s=gm.means_\n",
    "print(mu_s)\n",
    "print('----------------')\n",
    "var_s=gm.covariances_\n",
    "print(var_s)\n",
    "\n",
    "#target\n",
    "print('for target')\n",
    "gm = GaussianMixture(n_components=1,random_state=0).fit(data_target_unlabeled[['feat_0','feat_1']].append(data_target_labeled[['feat_0','feat_1']]))\n",
    "mu_t=gm.means_\n",
    "print(mu_t)\n",
    "print('----------------')\n",
    "var_t=gm.covariances_\n",
    "print(var_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40da46",
   "metadata": {},
   "source": [
    "The second approach yelds a better reults because the two distributions P_s(x) and P_t(x) are different due to the covariate shift. So, it is more likely that we will get a more accurate results when the parameters on each domain are estimated seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(g)\n",
    "sample_weights = np.array([(multivariate_normal.pdf(x, mean=[-1.05830525, -0.45760988], cov=[[ 0.76377201 , 0.30418792],[ 0.30418792 ,19.02092692]]) \n",
    "                           / multivariate_normal.pdf(x, mean=[1.14916266, 1.09731636], cov=[[ 3.70506986, -0.08005567],[-0.08005567 , 0.65845626]]))\n",
    "                           for x in np.array(data_source_train[['feat_0','feat_1']])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= AdaBoostClassifier()\n",
    "model.fit(data_source_train[['feat_0','feat_1']],data_source_train['class'], sample_weights)\n",
    "print(accuracy_score(data_target_test['class'], model.predict(data_target_test[['feat_0','feat_1']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add69313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the unwighted target lableded data\n",
    "model= AdaBoostClassifier()\n",
    "model.fit(data_source_train[['feat_0','feat_1']].append(data_target_labeled[['feat_0','feat_1']]),data_source_train['class'].append(data_target_labeled['class']), np.append(sample_weights,[1,1]))\n",
    "print(accuracy_score(data_target_test['class'], model.predict(data_target_test[['feat_0','feat_1']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd8603",
   "metadata": {},
   "source": [
    "The transfer learing approach performed better than the supervised learning approach because the source training points have been weighted to compenstate for the change in the density function in the two domains. The supervised approach trained the classifiers using the few target training points and the original source training point without any correction factor, and hence it is expected to perform worse that the transfer learning approach.\n",
    "\n",
    "We also notice that the best accuracy achieved was when the weighted  source training data and the few labled target training data were all utlized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df73c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
