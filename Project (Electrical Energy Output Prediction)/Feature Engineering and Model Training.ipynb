{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a36d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "import itertools as iter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset=pd.read_csv('Dataset.csv')\n",
    "X=Dataset[['AT','V','AP','RH']]\n",
    "y=Dataset[['PE']]\n",
    "X_dash,X_test,y_dash,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "# Outliers: assume that we will define an outlier as a sample that differes from the mean by a significant amount (let's say 2.5 standard deviations)\n",
    "feature=['AT','V','AP','RH']\n",
    "for j in range(4):\n",
    "    number_of_outliers=0\n",
    "    for i in Dataset[feature[j]]:\n",
    "        if i>=Dataset[feature[j]].mean()+3*Dataset[feature[j]].std()  or  i<=Dataset[feature[j]].mean()-3*Dataset[feature[j]].std():\n",
    "            number_of_outliers=number_of_outliers+1\n",
    "    print(feature[j],'has',number_of_outliers,'outliers')\n",
    "sns.boxplot(x=X_dash['AT'])\n",
    "plt.show()\n",
    "sns.boxplot(x=X_dash['V'])\n",
    "plt.show()\n",
    "sns.boxplot(x=X_dash['AP'])\n",
    "plt.show()\n",
    "sns.boxplot(x=X_dash['RH'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#study the correlation between variables and the prediction power of the feeatures\n",
    "corr = X_dash[['AT','V','AP','RH']].join(y_dash['PE']).corr()\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(corr, annot=True, annot_kws={'size':15})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model=DecisionTreeRegressor()\n",
    "model.fit(X_dash[['AT','V','AP','RH']],y_dash)\n",
    "features=pd.Series(model.feature_importances_,X_dash[['AT','V','AP','RH']].columns).sort_values(ascending=False)\n",
    "features.plot(kind='bar',title='Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train and validation sets splits\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_dash,y_dash,train_size=0.75,random_state=0)\n",
    "\n",
    "def pset(lst):\n",
    "    comb = (iter.combinations(lst, l) for l in range(len(lst) + 1))\n",
    "    res=list(iter.chain.from_iterable(comb))\n",
    "    return res\n",
    "\n",
    "\n",
    "#best subset selection\n",
    "c=pset(X_dash[['AT','V','AP','RH']])\n",
    "valadation_scores={}\n",
    "for i in range(1,16,1):\n",
    "    model=LinearRegression()\n",
    "    model.fit(X_train[list(c[i])],y_train)\n",
    "    y_train_pred=model.predict(X_train[list(c[i])])\n",
    "    y_val_pred=model.predict(X_val[list(c[i])])\n",
    "    valadation_scores.update({c[i]:mean_squared_error(y_val,y_val_pred)})\n",
    "\n",
    "best_val_score=min(valadation_scores, key=valadation_scores.get)\n",
    "print(valadation_scores)\n",
    "\n",
    "\n",
    "final_scores={}\n",
    "for i in range(1,16,1):\n",
    "    model=LinearRegression()\n",
    "        \n",
    "    scores=cross_val_score(model,X_dash[list(c[i])], y_dash,cv=5, scoring='neg_mean_squared_error')\n",
    "    final_scores.update({c[i]:scores.mean()})\n",
    "    \n",
    "best_feature_set=max(final_scores, key=final_scores.get)\n",
    "print(final_scores)\n",
    "print('best featrue subset are:',best_feature_set)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#study the interaction terms\n",
    "#add the interation terms to our feature sets\n",
    "X_dash_=pd.DataFrame([X_dash['AT'],X_dash['V'],X_dash['AP'],X_dash['RH'],X_dash['AT']*X_dash['V'],X_dash['AT']*X_dash['AP'],X_dash['AT']*X_dash['RH'],X_dash['V']*X_dash['AP'],X_dash['V']*X_dash['RH'],X_dash['AP']*X_dash['RH']])\n",
    "X_dash_=X_dash_.T\n",
    "X_dash_.columns=['AT','V','AP','RH','AT*V','AT*AP','AT*RH','V*AP','V*RH','AP*RH']\n",
    "\n",
    "X_test_=pd.DataFrame([X_test['AT'],X_test['V'],X_test['AP'],X_test['RH'],X_test['AT']*X_test['V'],X_test['AT']*X_test['AP'],X_test['AT']*X_test['RH'],X_test['V']*X_test['AP'],X_test['V']*X_test['RH'],X_test['AP']*X_test['RH']])\n",
    "X_test_=X_test_.T\n",
    "X_test_.columns=['AT','V','AP','RH','AT*V','AT*AP','AT*RH','V*AP','V*RH','AP*RH']\n",
    "\n",
    "#fit a linear regression model and obseve the p-value of the interaction terms to determine their significance\n",
    "X_dash_withconsnt= sm.add_constant(X_dash_[['AT','V','AP','RH','AT*V','AT*AP','AT*RH','V*AP','V*RH','AP*RH']])\n",
    "model = sm.OLS(y_dash, X_dash_withconsnt)\n",
    "model = model.fit()\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "X_dash_withconsnt= sm.add_constant(X_dash)\n",
    "model = sm.OLS(y_dash, X_dash_withconsnt)\n",
    "model = model.fit()\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model=DecisionTreeRegressor()\n",
    "model.fit(X_dash_,y_dash)\n",
    "features=pd.Series(model.feature_importances_,X_dash_.columns).sort_values(ascending=False)\n",
    "features.plot(kind='bar',title='Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "#now we will find the best set of interaction terms using a validation set.\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_dash_,y_dash,train_size=0.75,random_state=0)\n",
    "\n",
    "c=pset(X_dash_[['AT*V','AT*AP','AT*RH','V*AP','V*RH','AP*RH']])\n",
    "\n",
    "\n",
    "valadation_scores={}\n",
    "training_scores={}\n",
    "for i in range(1,64,1):\n",
    "    model=LinearRegression()\n",
    "    model.fit(X_train[['AT','V','AP','RH']].join(X_train[list(c[i])]),y_train)\n",
    "    y_train_pred=model.predict(X_train[['AT','V','AP','RH']].join(X_train[list(c[i])]))\n",
    "    y_val_pred=model.predict(X_val[['AT','V','AP','RH']].join(X_val[list(c[i])]))\n",
    "    training_scores.update({c[i]:mean_squared_error(y_train,y_train_pred)})\n",
    "    valadation_scores.update({c[i]:mean_squared_error(y_val,y_val_pred)})\n",
    "\n",
    "best_val_score=min(valadation_scores, key=valadation_scores.get)\n",
    "\n",
    "best_training_scores={}\n",
    "for j in range(1,7,1):\n",
    "    temp={}\n",
    "    for i in training_scores.keys():\n",
    "        if len(i)==j:\n",
    "            temp.update({i:training_scores[i]})\n",
    "    best_training_scores.update({j:min(temp, key=temp.get)})\n",
    "\n",
    "print('best fitted model for each d',best_training_scores)\n",
    "\n",
    "best_validation_scores={}\n",
    "for j in best_training_scores.values():\n",
    "    best_validation_scores.update({j:valadation_scores[j]})\n",
    "    \n",
    "best_feature_set=min(best_validation_scores, key=valadation_scores.get)\n",
    "print('validation error for each best fitted model with d features',best_validation_scores)\n",
    "\n",
    "\n",
    "\n",
    "# now we will use these features to learn multiple models that we learned from EE 660\n",
    "X_train=X_train[['AT','V','AP','RH']].join(X_train[list(best_feature_set)])\n",
    "X_val=X_val[['AT','V','AP','RH']].join(X_val[list(best_feature_set)])\n",
    "X_test=X_test_[['AT','V','AP','RH']].join(X_test_[list(best_feature_set)])\n",
    "X_dash=X_dash_[['AT','V','AP','RH']].join(X_dash_[list(best_feature_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2217329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso and Ridge\n",
    "#This is the code for the standardized case using validation set\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std=scaler.transform(X_train)\n",
    "X_val_std=scaler.transform(X_val)\n",
    "X_test_std=scaler.transform(X_test)\n",
    "\n",
    "valadation_scores={}\n",
    "for i in np.linspace(-10,10,21):\n",
    "    model=Lasso(alpha=2**i)\n",
    "    model.fit(X_train_std,y_train)\n",
    "    y_val_pred=model.predict(X_val_std)\n",
    "    valadation_scores.update({2**i:mean_squared_error(y_val,y_val_pred)})\n",
    "\n",
    "best_lambda_laaso=min(valadation_scores, key=valadation_scores.get)\n",
    "\n",
    "\n",
    "model=Lasso(alpha=best_lambda_laaso)\n",
    "model.fit(X_train_std,y_train)\n",
    "y_train_pred=model.predict(X_train_std)\n",
    "y_val_pred=model.predict(X_val_std)\n",
    "\n",
    "print('LASSOTrain MSE is',mean_squared_error(y_train,y_train_pred))\n",
    "print('LASSOValadation set MSE is:',mean_squared_error(y_val,y_val_pred))\n",
    "print('best lambda Lasso is:',best_lambda_laaso)\n",
    "plt.plot(np.linspace(-10,10,21),valadation_scores.values())\n",
    "plt.xlabel('regularization parameter')\n",
    "plt.ylabel('validation error')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "valadation_scores={}\n",
    "for i in np.linspace(-10,10,21):\n",
    "    model=Ridge(alpha=2**i)\n",
    "    model.fit(X_train_std,y_train)\n",
    "    y_val_pred=model.predict(X_val_std)\n",
    "    valadation_scores.update({2**i:mean_squared_error(y_val,y_val_pred)})\n",
    "\n",
    "best_lambda_Ridge=min(valadation_scores, key=valadation_scores.get)\n",
    "\n",
    "\n",
    "model=Ridge(alpha=best_lambda_Ridge)\n",
    "model.fit(X_train_std,y_train)\n",
    "y_train_pred=model.predict(X_train_std)\n",
    "y_val_pred=model.predict(X_val_std)\n",
    "\n",
    "print('RidgeTrain MSE is',mean_squared_error(y_train,y_train_pred))\n",
    "print('RidgeValadation set MSE is:',mean_squared_error(y_val,y_val_pred))\n",
    "print('best lambda ridge is:',best_lambda_Ridge)\n",
    "plt.plot(np.linspace(-10,10,21),valadation_scores.values())\n",
    "plt.xlabel('regularization parameter')\n",
    "plt.ylabel('validation error')\n",
    "plt.show()\n",
    "\n",
    "#study the variabity of diffrent training size for Lasso and Ridge\n",
    "best_lambdas_laaso=[]\n",
    "validation_scores=[]\n",
    "N=[0.1,0.2 ,0.3,0.4,0.5, 0.6,0.7,0.8, 0.9,0.999]\n",
    "for j in N:\n",
    "    valadation_scores={}\n",
    "    \n",
    "    for i in np.linspace(-10,10,21):\n",
    "        model=Lasso(alpha=2**i)\n",
    "        X_train_bagged,X_throw,y_train_bagged,y_throw=train_test_split(X_train_std,y_train,train_size=j ,random_state=0)\n",
    "        model.fit(X_train_bagged,y_train_bagged)\n",
    "        y_val_pred=model.predict(X_val_std)\n",
    "        valadation_scores.update({2**i:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "    best_lambda_laaso=min(valadation_scores, key=valadation_scores.get)\n",
    "    best_lambdas_laaso.append(best_lambda_laaso)\n",
    "\n",
    "\n",
    "    model=Lasso(alpha=best_lambda_laaso)\n",
    "    model.fit(X_train_bagged,y_train_bagged)\n",
    "    y_train_pred=model.predict(X_train_bagged)\n",
    "    validation_scores.append(mean_squared_error(y_val,model.predict(X_val_std)))\n",
    "    \n",
    "    \n",
    "plt.plot(N,best_lambdas_laaso);plt.xlabel('Training Size');plt.ylabel('best_lambdas_laaso')\n",
    "plt.show()\n",
    "plt.plot(N,validation_scores);plt.xlabel('Training Size');plt.ylabel('validation error')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_lambdas_Ridge=[]\n",
    "validation_scores=[]\n",
    "N=[0.1,0.2 ,0.3,0.4,0.5, 0.6,0.7,0.8, 0.9,0.999]\n",
    "for j in N:\n",
    "    valadation_scores={}\n",
    "    \n",
    "    for i in np.linspace(-10,10,21):\n",
    "        model=Ridge(alpha=2**i)\n",
    "        X_train_bagged,X_throw,y_train_bagged,y_throw=train_test_split(X_train_std,y_train,train_size=j ,random_state=0)\n",
    "        model.fit(X_train_bagged,y_train_bagged)\n",
    "        y_val_pred=model.predict(X_val_std)\n",
    "        valadation_scores.update({2**i:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "    best_lambda_Ridge=min(valadation_scores, key=valadation_scores.get)\n",
    "    best_lambdas_Ridge.append(best_lambda_Ridge)\n",
    "\n",
    "\n",
    "    model=Ridge(alpha=best_lambda_Ridge)\n",
    "    model.fit(X_train_bagged,y_train_bagged)\n",
    "    y_train_pred=model.predict(X_train_bagged)\n",
    "    validation_scores.append(mean_squared_error(y_val,model.predict(X_val_std)))\n",
    "    \n",
    "plt.plot(N,best_lambdas_Ridge);plt.xlabel('Training Size');plt.ylabel('best_lambdas_Ridge')\n",
    "plt.show()\n",
    "plt.plot(N,validation_scores);plt.xlabel('Training Size');plt.ylabel('validation error')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#study the robusness to the noise for Lasso and Ridge\n",
    "best_lambda_Lasso_mean=[]\n",
    "for it in np.arange(1,9,1):\n",
    "    best_lambdas_Lasso=[]\n",
    "    validation_scores=[]\n",
    "    noise_std=np.arange(0.01,0.09,0.01)\n",
    "    for j in noise_std:\n",
    "        valadation_scores={}\n",
    "    \n",
    "        for i in np.linspace(-10,10,21):\n",
    "            model=Lasso(alpha=2**i)\n",
    "            X_train_std_noisy=X_train_std+np.random.normal(0,j,[5740,7])\n",
    "            model.fit(X_train_std_noisy,y_train)\n",
    "            y_val_pred=model.predict(X_val_std)\n",
    "            valadation_scores.update({2**i:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "        best_lambda_Lasso=min(valadation_scores, key=valadation_scores.get)\n",
    "        best_lambdas_Lasso.append(best_lambda_Lasso)\n",
    "\n",
    "\n",
    "        model=Lasso(alpha=best_lambda_Lasso)\n",
    "        model.fit(X_train_std_noisy,y_train)\n",
    "        y_train_pred=model.predict(X_train_std_noisy)\n",
    "        validation_scores.append(mean_squared_error(y_val,model.predict(X_val_std)))\n",
    "        \n",
    "    \n",
    "    best_lambda_Lasso_mean.append(np.mean(best_lambdas_Lasso))\n",
    "\n",
    "plt.plot(noise_std,best_lambda_Lasso_mean,c='r');plt.xlabel('noise_std');plt.ylabel('best_lambdas_Lasso')\n",
    "plt.show()\n",
    "plt.plot(noise_std,validation_scores,c='r');plt.xlabel('noise_std');plt.ylabel('validation error')\n",
    "plt.show()\n",
    "\n",
    "best_lambda_Ridge_mean=[]\n",
    "for it in np.arange(1,9,1):\n",
    "    best_lambdas_Ridge=[]\n",
    "    validation_scores=[]\n",
    "    noise_std=np.arange(0.01,0.09,0.01)\n",
    "    for j in noise_std:\n",
    "        valadation_scores={}\n",
    "    \n",
    "        for i in np.linspace(-10,10,21):\n",
    "            model=Ridge(alpha=2**i)\n",
    "            X_train_std_noisy=X_train_std+np.random.normal(0,j,[5740,7])\n",
    "            model.fit(X_train_std_noisy,y_train)\n",
    "            y_val_pred=model.predict(X_val_std)\n",
    "            valadation_scores.update({2**i:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "        best_lambda_Ridge=min(valadation_scores, key=valadation_scores.get)\n",
    "        best_lambdas_Ridge.append(best_lambda_Ridge)\n",
    "\n",
    "\n",
    "        model=Ridge(alpha=best_lambda_Ridge)\n",
    "        model.fit(X_train_std_noisy,y_train)\n",
    "        y_train_pred=model.predict(X_train_std_noisy)\n",
    "        validation_scores.append(mean_squared_error(y_val,model.predict(X_val_std)))\n",
    "    \n",
    "    best_lambda_Ridge_mean.append(np.mean(best_lambdas_Ridge))\n",
    "\n",
    "plt.plot(noise_std,best_lambda_Ridge_mean,c='r');plt.xlabel('noise_std');plt.ylabel('best_lambdas_Ridge')\n",
    "plt.show()\n",
    "plt.plot(noise_std,validation_scores,c='r');plt.xlabel('noise_std');plt.ylabel('validation error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb131a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CART\n",
    "valadation_scores={}\n",
    "\n",
    "for i in range(20):\n",
    "    model=DecisionTreeRegressor(max_depth=i+1,criterion='mse',max_features=None,random_state=0)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_val_pred=model.predict(X_val)\n",
    "    valadation_scores.update({i+1:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "best_depth=min(valadation_scores, key=valadation_scores.get)\n",
    "\n",
    "\n",
    "model=DecisionTreeRegressor(max_depth=best_depth,criterion='mse',max_features=None,\n",
    "                            random_state=0)\n",
    "model.fit(X_train,y_train)\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_val_pred=model.predict(X_val)\n",
    "y_test_pred=model.predict(X_test)\n",
    "print('CART Train MSE is',mean_squared_error(y_train,y_train_pred))\n",
    "print('CART Valadation set MSE is:',mean_squared_error(y_val,y_val_pred))\n",
    "plt.plot(valadation_scores.keys(),valadation_scores.values())\n",
    "plt.xlabel('Depth of The Tree')\n",
    "plt.ylabel('Validation Error')\n",
    "plt.show()\n",
    "print('best depth is:',best_depth)\n",
    "\n",
    "#study the variabity of diffrent training size for CART\n",
    "best_depths=[]\n",
    "validation_scores=[]\n",
    "N=np.arange(0.01,0.99,0.01)\n",
    "for j in N:\n",
    "    valadation_scores={}\n",
    "    \n",
    "    for i in range(20):\n",
    "        model=DecisionTreeRegressor(max_depth=i+1,criterion='mse',max_features=None,random_state=0)\n",
    "        X_train_bagged,X_throw,y_train_bagged,y_throw=train_test_split(X_train,y_train,train_size=j ,random_state=0)\n",
    "        model.fit(X_train_bagged,y_train_bagged)\n",
    "        y_val_pred=model.predict(X_val)\n",
    "        valadation_scores.update({i+1:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "    best_depth=min(valadation_scores, key=valadation_scores.get)\n",
    "    best_depths.append(best_depth)\n",
    "\n",
    "\n",
    "    model=DecisionTreeRegressor(max_depth=best_depth,criterion='mse',max_features=None,random_state=0)\n",
    "    model.fit(X_train_bagged,y_train_bagged)\n",
    "    y_train_pred=model.predict(X_train_bagged)\n",
    "    validation_scores.append(mean_squared_error(y_val,model.predict(X_val)))\n",
    "    \n",
    "\n",
    "\n",
    "plt.plot(N,best_depths);plt.xlabel('Training Size');plt.ylabel('Best Depth')\n",
    "plt.show()\n",
    "plt.plot(N,validation_scores);plt.xlabel('Training Size');plt.ylabel('validation error')\n",
    "plt.show()\n",
    "\n",
    "#study the robusness to the noise for CART\n",
    "best_depths=[]\n",
    "validation_scores=[]\n",
    "noise_std=np.arange(0.01,0.1,0.01)\n",
    "for j in noise_std:\n",
    "    valadation_scores={}\n",
    "    \n",
    "    for i in range(20):\n",
    "        model=DecisionTreeRegressor(max_depth=i+1,criterion='mse',max_features=None,random_state=0)\n",
    "        X_train_std_noisy=X_train_std+np.random.normal(0,j,[5740,7])\n",
    "        model.fit(X_train_std_noisy,y_train)\n",
    "        y_val_pred=model.predict(X_val_std)\n",
    "        valadation_scores.update({i+1:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "    best_depth=min(valadation_scores, key=valadation_scores.get)\n",
    "    best_depths.append(best_depth)\n",
    "\n",
    "\n",
    "    model=DecisionTreeRegressor(max_depth=best_depth,criterion='mse',max_features=None,random_state=0)\n",
    "    model.fit(X_train_std_noisy,y_train)\n",
    "    y_train_pred=model.predict(X_train_std_noisy)\n",
    "    validation_scores.append(mean_squared_error(y_val,model.predict(X_val_std)))\n",
    "    \n",
    "\n",
    "\n",
    "plt.plot(noise_std,best_depths,c='r');plt.xlabel('noise_std');plt.ylabel('Best Depth')\n",
    "plt.show()\n",
    "plt.plot(noise_std,validation_scores,c='r');plt.xlabel('noise_std');plt.ylabel('validation error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ea9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "valadation_scores={}\n",
    "for i in range(2,100):\n",
    "    model=RandomForestRegressor(n_estimators=i,criterion='mse',max_depth=best_depth,\n",
    "                                max_features=None,random_state=0)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_val_pred=model.predict(X_val)\n",
    "    valadation_scores.update({i:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "best_tree_num=min(valadation_scores, key=valadation_scores.get)\n",
    "\n",
    "model=RandomForestRegressor(n_estimators=best_tree_num,criterion='mse',max_depth=best_depth,\n",
    "                            max_features=None,random_state=0)\n",
    "model.fit(X_train,y_train)\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_val_pred=model.predict(X_val)\n",
    "y_test_pred=model.predict(X_test)\n",
    "print('RF Train MSE is',mean_squared_error(y_train,y_train_pred))\n",
    "print('RF Valadation set MSE is:',mean_squared_error(y_val,y_val_pred))\n",
    "print('best tree numberis:',best_tree_num)\n",
    "plt.plot(valadation_scores.keys(),valadation_scores.values())\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Validation Error')\n",
    "plt.show()\n",
    "\n",
    "#study the variabity of diffrent training size for Random Forest\n",
    "def find_vectors_and_plots(B,bagg_size,max_depthh,max_featuress):\n",
    "    train_mean_error_rate=[]\n",
    "    train_std_error_rate=[]\n",
    "    validation_mean_error_rate=[]\n",
    "    validation_std_error_rate=[]\n",
    "\n",
    "\n",
    "    for i in range(B):\n",
    "        training_error_rate=[]\n",
    "        validation_error_rate=[]\n",
    "    \n",
    "        for j in range(10):\n",
    "            X_train_bagged,X_test_bagged,y_train_bagged,ytest_bagged=train_test_split(X_train,y_train,\n",
    "            train_size=bagg_size)\n",
    "            model=RandomForestRegressor(n_estimators=i+1,criterion='mse',max_depth=max_depthh,bootstrap=True,max_features=max_featuress)\n",
    "            \n",
    "            model.fit(X_train_bagged,y_train_bagged.values.ravel())\n",
    "            y_train_pred=model.predict(X_train)\n",
    "            y_val_pred=model.predict(X_val)\n",
    "        \n",
    "        \n",
    "            training_error_rate.append(mean_squared_error(y_train,y_train_pred))\n",
    "            validation_error_rate.append(mean_squared_error(y_val,y_val_pred))\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_mean_error_rate.append(np.array(training_error_rate).mean())\n",
    "        train_std_error_rate.append(np.array(training_error_rate).std())\n",
    "        validation_mean_error_rate.append(np.array(validation_error_rate).mean())\n",
    "        validation_std_error_rate.append(np.array(validation_error_rate).std())\n",
    "    \n",
    "    \n",
    "    x=np.arange(1,B+1)\n",
    "    plt.plot(x,train_mean_error_rate,label='training data')\n",
    "    plt.plot(x,validation_mean_error_rate,label='validation data')\n",
    "    plt.ylabel('mean error rate')\n",
    "    plt.xlabel('number of trees')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.plot(x,train_std_error_rate,label='training data')\n",
    "    plt.plot(x,validation_std_error_rate,label='validation data')\n",
    "    plt.ylabel('standard deviation of error rate')\n",
    "    plt.xlabel('number of trees')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print('the lowest validation error is when B=',np.argmin(validation_mean_error_rate)+1,'with validation error of:',np.array(validation_mean_error_rate).min(),'with standar deviation of',validation_std_error_rate[np.argmin(validation_mean_error_rate)])\n",
    "    \n",
    "\n",
    "N=[0.1,0.2 ,0.3,0.4,0.5, 0.6,0.7,0.8, 0.9,0.999]\n",
    "for j in N:\n",
    "    find_vectors_and_plots(50,j,10,3)\n",
    "    \n",
    "\n",
    "\n",
    "#study the robusness to the noise for Random Forest\n",
    "best_tree_nums=[]\n",
    "validation_scores=[]\n",
    "noise_std=np.arange(0.01,0.1,0.01)\n",
    "for j in noise_std:\n",
    "    valadation_scores={}\n",
    "    \n",
    "    for i in range(2,50):\n",
    "        model=RandomForestRegressor(n_estimators=i,criterion='mse',max_depth=best_depth,max_features=None,random_state=0)\n",
    "        X_train_std_noisy=X_train_std+np.random.normal(0,j,[5740,7])\n",
    "        model.fit(X_train_std_noisy,y_train)\n",
    "        y_val_pred=model.predict(X_val_std)\n",
    "        valadation_scores.update({i+1:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "    best_tree_num=min(valadation_scores, key=valadation_scores.get)\n",
    "    best_tree_nums.append(best_tree_num)\n",
    "\n",
    "\n",
    "    model=RandomForestRegressor(n_estimators=best_tree_num,criterion='mse',max_depth=best_depth,max_features=None,random_state=0)\n",
    "    model.fit(X_train_std_noisy,y_train)\n",
    "    y_train_pred=model.predict(X_train_std_noisy)\n",
    "    validation_scores.append(mean_squared_error(y_val,model.predict(X_val_std)))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "plt.plot(noise_std,best_tree_nums,c='r');plt.xlabel('noise_std');plt.ylabel('best_tree_number')\n",
    "plt.show()\n",
    "plt.plot(noise_std,validation_scores,c='r');plt.xlabel('noise_std');plt.ylabel('validation error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ada_boost\n",
    "valadation_scores={}\n",
    "for i in np.arange(0.1, 3, 0.1):\n",
    "    model=AdaBoostRegressor(DecisionTreeRegressor(max_depth=10,criterion='mse',\n",
    "                                                  max_features=None,random_state=0),\n",
    "                            n_estimators=48, learning_rate=i,random_state=0)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_val_pred=model.predict(X_val)\n",
    "    valadation_scores.update({i:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "best_learning_rate=min(valadation_scores, key=valadation_scores.get)\n",
    "\n",
    "model=AdaBoostRegressor(DecisionTreeRegressor(max_depth=10,criterion='mse',\n",
    "                                              max_features=None,random_state=0),\n",
    "                        n_estimators=48, learning_rate=0.8,random_state=0)\n",
    "model.fit(X_train,y_train)\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_val_pred=model.predict(X_val)\n",
    "y_test_pred=model.predict(X_test)\n",
    "print('Adaboost Train MSE is',mean_squared_error(y_train,y_train_pred))\n",
    "print('Adaboost Valadation set MSE is:',mean_squared_error(y_val,y_val_pred))\n",
    "print('best learning rate is:',best_learning_rate)\n",
    "plt.plot(valadation_scores.keys(),valadation_scores.values())\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Validation Error')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#study the variabity of diffrent training size for Ada boost\n",
    "best_learning_rates=[]\n",
    "validation_scores=[]\n",
    "N=np.arange(0.01,0.1,0.01)\n",
    "for j in N:\n",
    "    valadation_scores={}\n",
    "    \n",
    "    for i in np.arange(0.1, 3, 0.1):\n",
    "        model=AdaBoostRegressor(DecisionTreeRegressor(max_depth=10,criterion='mse',max_features=None,random_state=0),n_estimators=48, learning_rate=0.8,random_state=0)\n",
    "        X_train_bagged,X_throw,y_train_bagged,y_throw=train_test_split(X_train,y_train,train_size=j ,random_state=0)\n",
    "        model.fit(X_train_bagged,y_train_bagged)\n",
    "        y_val_pred=model.predict(X_val)\n",
    "        valadation_scores.update({i:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "    best_learning_rate=min(valadation_scores, key=valadation_scores.get)\n",
    "    best_learning_rates.append(best_learning_rate)\n",
    "\n",
    "\n",
    "    model=AdaBoostRegressor(DecisionTreeRegressor(max_depth=best_depth,criterion='mse',max_features=None,random_state=0),n_estimators=best_tree_num, learning_rate=best_learning_rate,random_state=0)\n",
    "    model.fit(X_train_bagged,y_train_bagged)\n",
    "    y_train_pred=model.predict(X_train_bagged)\n",
    "    validation_scores.append(mean_squared_error(y_val,model.predict(X_val)))\n",
    "    test_scores.append(mean_squared_error(y_test,model.predict(X_test)))\n",
    "    \n",
    "\n",
    "\n",
    "plt.plot(N,best_learning_rates);plt.xlabel('Training Size');plt.ylabel('best_learning_rates')\n",
    "plt.show()\n",
    "plt.plot(N,validation_scores);plt.xlabel('Training Size');plt.ylabel('validation error')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#study the robusness to the noise for Ada boost\n",
    "best_learning_rates=[]\n",
    "validation_scores=[]\n",
    "noise_std=np.arange(0.01,0.1,0.01)\n",
    "for j in noise_std:\n",
    "    valadation_scores={}\n",
    "    \n",
    "    for i in np.arange(0.1, 3, 0.1):\n",
    "        model=AdaBoostRegressor(DecisionTreeRegressor(max_depth=10,criterion='mse',\n",
    "                                                  max_features=None,random_state=0),\n",
    "                            n_estimators=48, learning_rate=0.8,random_state=0)\n",
    "        X_train_std_noisy=X_train_std+np.random.normal(0,j,[5740,7])\n",
    "        model.fit(X_train_std_noisy,y_train)\n",
    "        y_val_pred=model.predict(X_val_std)\n",
    "        valadation_scores.update({i:mean_squared_error(y_val,y_val_pred)})\n",
    "    \n",
    "    best_learning_rate=min(valadation_scores, key=valadation_scores.get)\n",
    "    best_learning_rates.append(best_learning_rate)\n",
    "\n",
    "\n",
    "    model=AdaBoostRegressor(DecisionTreeRegressor(max_depth=10,criterion='mse',\n",
    "                                                  max_features=None,random_state=0),\n",
    "                            n_estimators=48, learning_rate=0.8,random_state=0)\n",
    "    model.fit(X_train_std_noisy,y_train)\n",
    "    y_train_pred=model.predict(X_train_std_noisy)\n",
    "    validation_scores.append(mean_squared_error(y_val,model.predict(X_val_std)))\n",
    "    test_scores.append(mean_squared_error(y_test,model.predict(X_test_std)))\n",
    "    \n",
    "\n",
    "\n",
    "plt.plot(noise_std,best_learning_rates,c='r');plt.xlabel('noise_std');plt.ylabel('best learning rate')\n",
    "plt.show()\n",
    "plt.plot(noise_std,validation_scores,c='r');plt.xlabel('noise_std');plt.ylabel('validation error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dbf6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen model\n",
    "model=AdaBoostRegressor(DecisionTreeRegressor(max_depth=10,criterion='mse',\n",
    "                                              max_features=None,random_state=0),\n",
    "                        n_estimators=48, learning_rate=0.8,random_state=0)\n",
    "model.fit(X_train,y_train)\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_val_pred=model.predict(X_val)\n",
    "y_test_pred=model.predict(X_test)\n",
    "print('Adaboost Train MSE is',mean_squared_error(y_train,y_train_pred))\n",
    "print('Adaboost Test MSE is:',mean_squared_error(y_test,y_test_pred))\n",
    "print('R2 score error is:',1 - r2_score(y_test, y_test_pred))\n",
    "\n",
    "\n",
    "#plot the regression functions\n",
    "model.fit(X_train[['AT']], y_train.values.ravel())\n",
    "predict_x = model.predict(X_train[['AT']])\n",
    "predict_test = model.predict(X_test[['AT']])\n",
    "X_grid = np.arange(min(np.array(X_train[['AT']])), max(np.array(X_train[['AT']])), 0.001)\n",
    "\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "\n",
    "        \n",
    "plt.scatter(X_train[['AT']], y_train, color='blue', label='training data points')\n",
    "        \n",
    "plt.plot(X_grid, model.predict(X_grid), color='green', label='regression function')\n",
    "plt.title('Ada boost regression')\n",
    "plt.xlabel('Ambient Temperature (AT)')\n",
    "plt.ylabel('Elecatical Energy (PE)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "model.fit(X_train[['V']], y_train.values.ravel())\n",
    "predict_x = model.predict(X_train[['V']])\n",
    "predict_test = model.predict(X_test[['V']])\n",
    "X_grid = np.arange(min(np.array(X_train[['V']])), max(np.array(X_train[['V']])), 0.001)\n",
    "\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "\n",
    "        \n",
    "plt.scatter(X_train[['V']], y_train, color='blue', label='training data points')\n",
    "        \n",
    "plt.plot(X_grid, model.predict(X_grid), color='green', label='regression function')\n",
    "plt.title('Ada boost regression')\n",
    "plt.xlabel('Exhaust Vacuum (V)')\n",
    "plt.ylabel('Elecatical Energy (PE)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
