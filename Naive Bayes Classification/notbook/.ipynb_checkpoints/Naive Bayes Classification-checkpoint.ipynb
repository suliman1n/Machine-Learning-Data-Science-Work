{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GywHxuEMAkQ"
   },
   "source": [
    "### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "txg3M9gmvJSl"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, auc,confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nf_QDRzuL-5_"
   },
   "source": [
    "### To define the Headings of the Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DsIoobjEjCUN",
    "outputId": "d70fa19f-9b76-4e98-f327-a89f998af82c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ID, Target, Time to recurrence, x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28, x29, dia, lym',\n",
       " 35)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Cols = ['ID', 'Target', 'Time to recurrence'] + \\\n",
    "        [f'x{i}' for i in range(30)] + ['dia', 'lym']\n",
    "Cols_ = ', '.join(Cols)\n",
    "Cols_, len(Cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12NIgJWOMF3F"
   },
   "source": [
    "### Converting the data file into csv for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q8PpDMha2dAt"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wpbc.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1b853899fa0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wpbc.data'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'xyz.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCols_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wpbc.data'"
     ]
    }
   ],
   "source": [
    "with open('wpbc.data') as myfile:\n",
    "    with open('xyz.csv', 'w') as f:\n",
    "        f.write(Cols_)\n",
    "        f.write('\\n')\n",
    "        for line in myfile.readlines():\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xnP4ehuMMEn"
   },
   "source": [
    "### reading the Data using the Pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "3kxP9Nd9f3PD",
    "outputId": "f296b44c-4fb6-4638-c8e2-2400ddca39c9"
   },
   "outputs": [],
   "source": [
    "Data = pd.read_csv('xyz.csv')\n",
    "print(Data.shape)\n",
    "Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcbjAbDAMRfs"
   },
   "source": [
    "## (a)(b)\n",
    "Seperating first 130 Non-recurrent and first 37 Recurrent from the data to form Train Dataset\n",
    "\n",
    "The Entry No #197 is also added to the Train set as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "zCjdPSAYjfsf",
    "outputId": "07cd1a26-7398-443b-8f62-0dc502409404"
   },
   "outputs": [],
   "source": [
    "D1 = Data[Data[' Target'] == 'N']\n",
    "D2 = Data[Data[' Target'] == 'R']\n",
    "Train = pd.concat([D1[:130], D2[:37]]).append(Data.iloc[196])\n",
    "Test = pd.concat([D1[130:], D2[37:]]).drop(196)\n",
    "print(D1.shape, D2.shape, Train.shape, Test.shape)\n",
    "Train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbKcMMl4MjQU"
   },
   "source": [
    "## (c):\n",
    "\n",
    "Imputing the datapoints where the data is not available or indicated as '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_uGPwf9lzuw",
    "outputId": "e5877511-768c-4d53-8e97-bd6a42ed710e"
   },
   "outputs": [],
   "source": [
    "print('Before Imputation', (Train[' lym'] == '?').sum())\n",
    "for col in Data.columns:\n",
    "    impute = np.median(Train[col] != '?')\n",
    "    Train[col] = Train[col].replace(['?'], impute)\n",
    "    Test[col] = Test[col].replace(['?'], impute)\n",
    "print('After Imputation', (Train[' lym'] == '?').sum())\n",
    "\n",
    "Train.loc[Train[' lym']==impute]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCOQIcJaMtYT"
   },
   "source": [
    "## Seperating  X (Datapoints) and Y (Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfSCjDfgrVjs",
    "outputId": "56840a02-b83a-468b-c0d7-d9acb3b82efb"
   },
   "outputs": [],
   "source": [
    "Y_train = Train[' Target']\n",
    "Y_test = Test[' Target']\n",
    "X_Train = Train.drop(['ID', ' Target', ' Time to recurrence'], axis=1)\n",
    "X_Test = Test.drop(['ID', ' Target', ' Time to recurrence'], axis=1)\n",
    "X_Train.shape, X_Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYWHtGHIM2nM"
   },
   "source": [
    "### Using Lable encoding to convert the probelm into binary {'0','1'} Classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgwPcVdaz-RS"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(Y_train)\n",
    "Y_Train = le.transform(Y_train)\n",
    "Y_Test = le.transform(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KenEO13mNBpp"
   },
   "source": [
    "## (d)(i):\n",
    "Gaussian Naive bayes with weighted probabilities as required in the Question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6y03zJ0cuKjb",
    "outputId": "f06e48b9-7eef-4f2f-945e-e7c4d56c925e"
   },
   "outputs": [],
   "source": [
    "p = Y_Train.sum()/len(Y_Train)\n",
    "print(f'The value of p is {p} as the dataset is unbalanced')\n",
    "clf = GaussianNB(priors=[1-p, p])\n",
    "clf.fit(X_Train, Y_Train)\n",
    "\n",
    "Y_train_pred = clf.predict_proba(X_Train)[:,1]\n",
    "Y_test_pred = clf.predict_proba(X_Test)[:,1]\n",
    "\n",
    "tr_fpr, tr_tpr, tr_thr = roc_curve(Y_Train, Y_train_pred)\n",
    "tst_fpr, tst_tpr, te_thr = roc_curve(Y_Test, Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78641N4_NRb1"
   },
   "source": [
    "#### The 'roc_curve' of sklearn gives the fpr, tpr and thresold values for the given Ground truth and predicted values of Target\n",
    "\n",
    "and 'auc' function gives area under curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "o7uhtOSPuMrt",
    "outputId": "d9f881da-3eb9-4f23-9d55-a679893e7cf3"
   },
   "outputs": [],
   "source": [
    "plt.plot(tr_fpr, tr_tpr, label=\"Train AUC =\"+str(auc(tr_fpr, tr_tpr)))\n",
    "plt.plot(tst_fpr, tst_tpr, label=\"Test AUC =\"+str(auc(tst_fpr, tst_tpr)))\n",
    "plt.grid() ; plt.legend()\n",
    "plt.xlabel(\"fpr\") ;plt.ylabel(\"tpr\") ;plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PyhmOoDN0Hb"
   },
   "source": [
    "### The 'confusion_matrix' of sklearn gives the confusion_matrix for the given ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAjxBSkk5A_k",
    "outputId": "b7854025-8b43-4be3-f8dc-a9350427d41e"
   },
   "outputs": [],
   "source": [
    "def predict_(proba, threshould):\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshould: predictions.append(1)\n",
    "        else:             predictions.append(0)\n",
    "    return predictions\n",
    "\n",
    "print(\"Train confusion matrix\")\n",
    "print(confusion_matrix(Y_Train, predict_(Y_train_pred, 0.5)))\n",
    "print(\"-\"*100)\n",
    "print(\"Test confusion matrix\")\n",
    "print(confusion_matrix(Y_Test, predict_(Y_test_pred, 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zddvj1eOBIo"
   },
   "source": [
    "### The 'precision_recall_fscore_support' of Sklearn gives the values as the name indicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HbmsDb36CZR",
    "outputId": "aaafba18-0f30-4b9c-edc9-f406676af03e"
   },
   "outputs": [],
   "source": [
    "Tr = precision_recall_fscore_support(Y_Train, predict_(Y_train_pred, 0.5))\n",
    "Te = precision_recall_fscore_support(Y_Test, predict_(Y_test_pred, 0.5))\n",
    "\n",
    "print(\"for Train dataset\")\n",
    "print(\"-\"*60)\n",
    "print('Precisions for CLasses \"N\" and \"R\" are : ', Tr[0])\n",
    "print('Recalls for CLasses \"N\" and \"R\" are : ', Tr[1])\n",
    "print('Fscores for CLasses \"N\" and \"R\" are : ', Tr[2])\n",
    "\n",
    "print(\"=\"*75)\n",
    "print(\"for Test dataset\")\n",
    "print(\"-\"*60)\n",
    "print('Precisions for CLasses \"N\" and \"R\" are : ', Te[0])\n",
    "print('Recalls for CLasses \"N\" and \"R\" are : ', Te[1])\n",
    "print('Fscores for CLasses \"N\" and \"R\" are : ', Te[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juOgvkWNA2z8"
   },
   "source": [
    "# (d)(ii) Balancing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5inFBGZP8CSb",
    "outputId": "dbfb8bb6-0755-4641-ce20-8ae6484072c4"
   },
   "outputs": [],
   "source": [
    "#!pip install -U imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "oversample = SMOTE(sampling_strategy={'R': 90}) #the default k value for KNN is 5\n",
    "X_Train, Y_train = oversample.fit_resample(X_Train, Y_train)\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy={'N': 90})\n",
    "X_Train, Y_train = undersample.fit_resample(X_Train, Y_train)\n",
    "\n",
    "X_Train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDlHIIAqBJzm"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(Y_train)\n",
    "Y_Train = le.transform(Y_train)\n",
    "Y_Test = le.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "foXI-bwsBGAJ",
    "outputId": "4d60625b-8dd3-4d9f-b019-6f391935b0a2"
   },
   "outputs": [],
   "source": [
    "p = Y_Train.sum()/len(Y_Train)\n",
    "print(f'The value of p is {p} as the dataset is balanced')\n",
    "clf = GaussianNB(priors=[1-p, p])\n",
    "clf.fit(X_Train, Y_Train)\n",
    "\n",
    "Y_train_pred = clf.predict_proba(X_Train)[:,1]\n",
    "Y_test_pred = clf.predict_proba(X_Test)[:,1]\n",
    "\n",
    "tr_fpr, tr_tpr, tr_thr = roc_curve(Y_Train, Y_train_pred)\n",
    "tst_fpr, tst_tpr, te_thr = roc_curve(Y_Test, Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "oMBsH8uVBfc7",
    "outputId": "3c81d754-4bea-43fd-d07b-7557bda9da54"
   },
   "outputs": [],
   "source": [
    "plt.plot(tr_fpr, tr_tpr, label=\"Train AUC =\"+str(auc(tr_fpr, tr_tpr)))\n",
    "plt.plot(tst_fpr, tst_tpr, label=\"Test AUC =\"+str(auc(tst_fpr, tst_tpr)))\n",
    "plt.grid() ; plt.legend()\n",
    "plt.xlabel(\"fpr\") ;plt.ylabel(\"tpr\") ;plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GoJnS40mBimm",
    "outputId": "b92af54c-eefe-49fc-9c13-da6a2fd144b2"
   },
   "outputs": [],
   "source": [
    "def predict_(proba, threshould):\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshould: predictions.append(1)\n",
    "        else:             predictions.append(0)\n",
    "    return predictions\n",
    "\n",
    "print(\"Train confusion matrix\")\n",
    "print(confusion_matrix(Y_Train, predict_(Y_train_pred, 0.5)))\n",
    "print(\"-\"*100)\n",
    "print(\"Test confusion matrix\")\n",
    "print(confusion_matrix(Y_Test, predict_(Y_test_pred, 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fbc0_iXPBkjb",
    "outputId": "ab453be3-99b3-4b68-bbf0-9fcc79d00e70"
   },
   "outputs": [],
   "source": [
    "Tr = precision_recall_fscore_support(Y_Train, predict_(Y_train_pred, 0.5))\n",
    "Te = precision_recall_fscore_support(Y_Test, predict_(Y_test_pred, 0.5))\n",
    "\n",
    "print(\"for Train dataset\")\n",
    "print(\"-\"*60)\n",
    "print('Precisions for CLasses \"N\" and \"R\" are : ', Tr[0])\n",
    "print('Recalls for CLasses \"N\" and \"R\" are : ', Tr[1])\n",
    "print('Fscores for CLasses \"N\" and \"R\" are : ', Tr[2])\n",
    "\n",
    "print(\"=\"*75)\n",
    "print(\"for Test dataset\")\n",
    "print(\"-\"*60)\n",
    "print('Precisions for CLasses \"N\" and \"R\" are : ', Te[0])\n",
    "print('Recalls for CLasses \"N\" and \"R\" are : ', Te[1])\n",
    "print('Fscores for CLasses \"N\" and \"R\" are : ', Te[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PsBf4YxB1qJ"
   },
   "source": [
    "# Observation\n",
    "\n",
    "\n",
    "\n",
    "*   The Test-AUC score improved from 0.555 to 0.603\n",
    "*   The Test- Precision  improved from [0.7 0.3] to [0.72222222 0.33333333]\n",
    "* The Test- Recall  changed from [0.66666667 0.33333333] to [0.61904762 0.44444444]\n",
    "* The Test- FScores  chenged from [0.68292683 0.31578947] to [0.66666667 0.38095238]\n",
    "\n",
    "\n",
    "as observed, there were some minor improvements after using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SMOTE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
