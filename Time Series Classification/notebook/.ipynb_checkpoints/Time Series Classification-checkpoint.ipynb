{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660ce420",
   "metadata": {},
   "source": [
    "# <center> Homework 5  <center/>\n",
    "### Import the required libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb42449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import statistics as stat   \n",
    "import seaborn as sns\n",
    "import pdb\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from patsy import dmatrices\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.linear_model import LogisticRegressionCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeabb3f0",
   "metadata": {},
   "source": [
    "# load each instance to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afc500c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'dataHW4/bending1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-19fd686c9725>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mSubfolderName\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'bending1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bending2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cycling'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lying'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sitting'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'standing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'walking'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m## There are seven subfolders in our dataset eg. bending1, bending2, ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataHW4/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mSubfolderName\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## accessing each file from each subfolder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'dataHW4/bending1'"
     ]
    }
   ],
   "source": [
    "Data={}\n",
    "for SubfolderName in ['bending1', 'bending2', 'cycling', 'lying', 'sitting', 'standing', 'walking']: ## There are seven subfolders in our dataset eg. bending1, bending2, ...  \n",
    "    files = os.listdir(\"C:\\Users\\Black Tornado0\\Desktop\\Git\\homework5-suliman1ndataHW4/\" + SubfolderName) ## accessing each file from each subfolder\n",
    "     \n",
    "    for file in files:  \n",
    "        ## dataset4 in bending2 has beed modified using excel to be similar to the others. \n",
    "        clmns = ['time','avg_RSS12', 'var_RSS12', 'avg_RSS13', 'var_RSS13', 'avg_RSS23', 'var_RSS23']\n",
    "        Data[SubfolderName + \"_\" + file] = pd.read_csv(\"dataHW4/\" + SubfolderName + \"/\" + file, delimiter=\",\", skiprows=5, names=clmns)\n",
    "        # we skip 5 rows in the excel file, and read the remaining rows into a dataframe\n",
    "Data['bending2_dataset4.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99361141",
   "metadata": {},
   "source": [
    "### (b). Keep datasets 1 and 2 in folders bending1 and bending 2, as well as datasets 1,2, and 3 in other folders as test data and other datasets as train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {'bending1_dataset1.csv':Data['bending1_dataset1.csv'],'bending1_dataset2.csv':Data['bending1_dataset2.csv'],\n",
    "            'bending2_dataset1.csv':Data['bending2_dataset1.csv'] ,'bending2_dataset2.csv':Data['bending2_dataset2.csv'],\n",
    "            'cycling_dataset1.csv':Data['cycling_dataset1.csv'],'cycling_dataset2.csv':Data['cycling_dataset2.csv'],'cycling_dataset3.csv':Data['cycling_dataset3.csv'],\n",
    "            'walking_dataset1.csv':Data['walking_dataset1.csv'],'walking_dataset2.csv':Data['walking_dataset2.csv'],'walking_dataset3.csv':Data['walking_dataset3.csv'],\n",
    "            'sitting_dataset1.csv':Data['sitting_dataset1.csv'],'sitting_dataset2.csv':Data['sitting_dataset2.csv'],'sitting_dataset3.csv':Data['sitting_dataset3.csv'],\n",
    "            'standing_dataset1.csv':Data['standing_dataset1.csv'],'standing_dataset2.csv':Data['standing_dataset2.csv'],'standing_dataset3.csv':Data['standing_dataset3.csv'],\n",
    "            'lying_dataset1.csv':Data['lying_dataset1.csv'],'lying_dataset2.csv':Data['lying_dataset2.csv'],'lying_dataset3.csv':Data['lying_dataset3.csv']}\n",
    "\n",
    "\n",
    "train_data={'bending1_dataset3.csv':Data['bending1_dataset3.csv'],'bending1_dataset4.csv':Data['bending1_dataset4.csv'],'bending1_dataset5.csv':Data['bending1_dataset5.csv'],'bending1_dataset6.csv':Data['bending1_dataset6.csv'],'bending1_dataset7.csv':Data['bending1_dataset7.csv'],\n",
    "           'bending2_dataset3.csv':Data['bending2_dataset3.csv'],'bending2_dataset4.csv':Data['bending2_dataset4.csv'],'bending2_dataset5.csv':Data['bending2_dataset5.csv'],'bending2_dataset6.csv':Data['bending2_dataset6.csv'],\n",
    "           'cycling_dataset4.csv':Data['cycling_dataset4.csv'],'cycling_dataset5.csv':Data['cycling_dataset5.csv'],'cycling_dataset6.csv':Data['cycling_dataset6.csv'],'cycling_dataset7.csv':Data['cycling_dataset7.csv'],'cycling_dataset8.csv':Data['cycling_dataset8.csv'],'cycling_dataset9.csv':Data['cycling_dataset9.csv'],'cycling_dataset10.csv':Data['cycling_dataset10.csv'],'cycling_dataset11.csv':Data['cycling_dataset11.csv'],'cycling_dataset12.csv':Data['cycling_dataset12.csv'],'cycling_dataset13.csv':Data['cycling_dataset13.csv'],'cycling_dataset14.csv':Data['cycling_dataset14.csv'],'cycling_dataset15.csv':Data['cycling_dataset15.csv'],\n",
    "           'walking_dataset4.csv':Data['walking_dataset4.csv'],'walking_dataset5.csv':Data['walking_dataset5.csv'],'walking_dataset6.csv':Data['walking_dataset6.csv'],'walking_dataset7.csv':Data['walking_dataset7.csv'],'walking_dataset8.csv':Data['walking_dataset8.csv'],'walking_dataset9.csv':Data['walking_dataset9.csv'],'walking_dataset10.csv':Data['walking_dataset10.csv'],'walking_dataset11.csv':Data['walking_dataset11.csv'],'walking_dataset12.csv':Data['walking_dataset12.csv'],'walking_dataset13.csv':Data['walking_dataset13.csv'],'walking_dataset14.csv':Data['walking_dataset14.csv'],'walking_dataset15.csv':Data['walking_dataset15.csv'],\n",
    "           'sitting_dataset4.csv':Data['sitting_dataset4.csv'],'sitting_dataset5.csv':Data['sitting_dataset5.csv'],'sitting_dataset6.csv':Data['sitting_dataset6.csv'],'sitting_dataset7.csv':Data['sitting_dataset7.csv'],'sitting_dataset8.csv':Data['sitting_dataset8.csv'],'sitting_dataset9.csv':Data['sitting_dataset9.csv'],'sitting_dataset10.csv':Data['sitting_dataset10.csv'],'sitting_dataset11.csv':Data['sitting_dataset11.csv'],'sitting_dataset12.csv':Data['sitting_dataset12.csv'],'sitting_dataset13.csv':Data['sitting_dataset13.csv'],'sitting_dataset14.csv':Data['sitting_dataset14.csv'],'sitting_dataset15.csv':Data['sitting_dataset15.csv'],\n",
    "           'standing_dataset4.csv':Data['standing_dataset4.csv'],'standing_dataset5.csv':Data['standing_dataset5.csv'],'standing_dataset6.csv':Data['standing_dataset6.csv'],'standing_dataset7.csv':Data['standing_dataset7.csv'],'standing_dataset8.csv':Data['standing_dataset8.csv'],'standing_dataset9.csv':Data['standing_dataset9.csv'],'standing_dataset10.csv':Data['standing_dataset10.csv'],'standing_dataset11.csv':Data['standing_dataset11.csv'],'standing_dataset12.csv':Data['standing_dataset12.csv'],'standing_dataset13.csv':Data['standing_dataset13.csv'],'standing_dataset14.csv':Data['standing_dataset14.csv'],'standing_dataset15.csv':Data['standing_dataset15.csv'],\n",
    "           'lying_dataset4.csv':Data['lying_dataset4.csv'],'lying_dataset5.csv':Data['lying_dataset5.csv'],'lying_dataset6.csv':Data['lying_dataset6.csv'],'lying_dataset7.csv':Data['lying_dataset7.csv'],'lying_dataset8.csv':Data['lying_dataset8.csv'],'lying_dataset9.csv':Data['lying_dataset9.csv'],'lying_dataset10.csv':Data['lying_dataset10.csv'],'lying_dataset11.csv':Data['lying_dataset11.csv'],'lying_dataset12.csv':Data['lying_dataset12.csv'],'lying_dataset13.csv':Data['lying_dataset13.csv'],'lying_dataset14.csv':Data['lying_dataset14.csv'],'lying_dataset15.csv':Data['lying_dataset15.csv']}\n",
    "\n",
    "test_data['lying_dataset1.csv']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a3cae",
   "metadata": {},
   "source": [
    "### c(i) Research what types of time-domain features are usually used in time series classification and list them\n",
    "\n",
    "There are a number of time-domain features used in time series classifications. A number of these features are listed below: \n",
    "1- Minimum\n",
    "\n",
    "2- maximum \n",
    "\n",
    "3- mean \n",
    "\n",
    "4- first quartile\n",
    "\n",
    "5- third quartile\n",
    "\n",
    "6- Standard Deviation \n",
    "\n",
    "7- Coeffiecient of skewness \n",
    "\n",
    "8- Coeffiecient of kurtosis \n",
    "\n",
    "9-Range\n",
    "\n",
    "10-inter-quartile range\n",
    "\n",
    "11-median\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d44b7c",
   "metadata": {},
   "source": [
    "### c(ii) Extract the time-domain features minimum, maximum, mean, median, stan-dard deviation, first quartile, and third quartile for all of the 6 time series in each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances=[]\n",
    "ith_instance=[]\n",
    "df=list(Data.values())\n",
    "for i in range(88):\n",
    "    \n",
    "    ith_instance=[min(df[i]['avg_RSS12']),\n",
    "    max(df[i]['avg_RSS12']),\n",
    "    stat.mean(df[i]['avg_RSS12']),\n",
    "    stat.median(df[i]['avg_RSS12']),\n",
    "    df[i]['avg_RSS12'].std(),\n",
    "    df[i]['avg_RSS12'].quantile(0.25),\n",
    "    df[i]['avg_RSS12'].quantile(0.75),\n",
    "    min(df[i]['var_RSS12']),\n",
    "    max(df[i]['var_RSS12']),\n",
    "    stat.mean(df[i]['var_RSS12']),\n",
    "    stat.median(df[i]['var_RSS12']),\n",
    "    df[i]['var_RSS12'].std(),\n",
    "    df[i]['var_RSS12'].quantile(0.25),\n",
    "    df[i]['var_RSS12'].quantile(0.75),\n",
    "    min(df[i]['avg_RSS13']),\n",
    "    max(df[i]['avg_RSS13']),\n",
    "    stat.mean(df[i]['avg_RSS13']),\n",
    "    stat.median(df[i]['avg_RSS13']),\n",
    "    df[i]['avg_RSS13'].std(),\n",
    "    df[i]['avg_RSS13'].quantile(0.25),\n",
    "    df[i]['avg_RSS13'].quantile(0.75),\n",
    "    min(df[i]['var_RSS13']),\n",
    "    max(df[i]['var_RSS13']),\n",
    "    stat.mean(df[i]['var_RSS13']),\n",
    "    stat.median(df[i]['var_RSS13']),\n",
    "    df[i]['var_RSS13'].std(),\n",
    "    df[i]['var_RSS13'].quantile(0.25),\n",
    "    df[i]['var_RSS13'].quantile(0.75),\n",
    "    min(df[i]['avg_RSS23']),\n",
    "    max(df[i]['avg_RSS23']),\n",
    "    stat.mean(df[i]['avg_RSS23']),\n",
    "    stat.median(df[i]['avg_RSS23']),\n",
    "    df[i]['avg_RSS23'].std(),\n",
    "    df[i]['avg_RSS23'].quantile(0.25),\n",
    "    df[i]['avg_RSS23'].quantile(0.75),\n",
    "    min(df[i]['var_RSS23']),\n",
    "    max(df[i]['var_RSS23']),\n",
    "    stat.mean(df[i]['var_RSS23']),\n",
    "    stat.median(df[i]['var_RSS23']),\n",
    "    df[i]['var_RSS23'].std(),\n",
    "    df[i]['var_RSS23'].quantile(0.25),\n",
    "    df[i]['var_RSS23'].quantile(0.75)]\n",
    "    instances.append(ith_instance)\n",
    "\n",
    "df_instances=pd.DataFrame(instances,columns=['min1','max1','mean1','median1','standard deviation1','1st quart1','3rd quart1','min2','max2','mean2','median2','standard deviation2','1st quart2','3rd quart2','min3','max3','mean3','median3','standard deviation3','1st quart3','3rd quart3','min4','max4','mean4','median4','standard deviation4','1st quart4','3rd quart4','min5','max5','mean5','median5','standard deviation5','1st quart5','3rd quart5','min6','max6','mean6','median6','standard deviation6','1st quart6','3rd quart6'])\n",
    "\n",
    "df_instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10710245",
   "metadata": {},
   "source": [
    "### c(iii) Estimate the standard deviation of each of the time-domain features you extracted from the data. Then, use Pythonâ€™s bootstrapped or any other method to build a 90% bootsrap confidence interval for the standard deviation of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "confInterval = {}\n",
    "for features in df_instances.columns:\n",
    "    sD = []\n",
    "    for i in range(0,2000):\n",
    "        sample = df_instances[features].sample(n=88)\n",
    "        stanDev = sample.std()\n",
    "        sD.append(stanDev)\n",
    "    sD.sort()\n",
    "    ## We can get 90% confidence interval by droping 5% lowest values and 5% greatest values.\n",
    "    \n",
    "    \n",
    "    confInterval[features] = [np.percentile(sD, 0.05),np.percentile(sD, 0.95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b410bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"90% confidence interval of each  feature is: \\n\")\n",
    "confInterval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2592a652",
   "metadata": {},
   "source": [
    "### c(iv) Use your judgement to select the three most important time-domain features\n",
    "I think the best features to be kept are  1st quartile, median and 3rd quartile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802524d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will keep 1st quartile, median and 3rd qurtile. We can use training dataset to take out these features. \n",
    "\n",
    "\n",
    "df_KeptData = df_instances[[\"1st quart1\", \"median1\", \"3rd quart1\", \"1st quart2\", \"median2\", \"3rd quart2\", \"1st quart6\", \"median6\", \"3rd quart6\"]]\n",
    "df_KeptData[\"bending\"] = [1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]  \n",
    "\n",
    "# 1 for bending, 0 for other activities\n",
    "df_KeptData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c762a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_KeptData, hue=\"bending\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-peter",
   "metadata": {},
   "source": [
    "# HW5: a(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {}\n",
    "import math\n",
    "\n",
    "for file, dataframe in Data.items():\n",
    "    length=int(len(dataframe))\n",
    "    length_2=math.floor(length/2)\n",
    "    \n",
    "    if (length_2%2==0):\n",
    "        avg_RSS12 = dataframe[\"avg_RSS12\"][0:int(length_2)]\n",
    "        avg_RSS12_1 = dataframe[\"avg_RSS12\"][int(length_2):]\n",
    "        var_RSS12 = dataframe[\"var_RSS12\"][0:int(length_2)]\n",
    "        var_RSS12_1 = dataframe[\"var_RSS12\"][int(length_2):]\n",
    "        avg_RSS13 = dataframe[\"avg_RSS13\"][0:int(length_2)]\n",
    "        avg_RSS13_1 = dataframe[\"avg_RSS13\"][int(length_2):]\n",
    "        var_RSS13 = dataframe[\"var_RSS13\"][0:int(length_2)]\n",
    "        var_RSS13_1 = dataframe[\"var_RSS13\"][int(length_2):]\n",
    "        avg_RSS23 = dataframe[\"avg_RSS23\"][0:int(length_2)]\n",
    "        avg_RSS23_1 = dataframe[\"avg_RSS23\"][int(length_2):]\n",
    "        var_RSS23 = dataframe[\"var_RSS23\"][0:int(length_2)]\n",
    "        var_RSS23_1 = dataframe[\"var_RSS23\"][int(length_2):]\n",
    "    else:\n",
    "        avg_RSS12 = dataframe[\"avg_RSS12\"][0:int(length_2)]\n",
    "        avg_RSS12_1 = dataframe[\"avg_RSS12\"][int(length_2)+1:]\n",
    "        var_RSS12 = dataframe[\"var_RSS12\"][0:int(length_2)]\n",
    "        var_RSS12_1 = dataframe[\"var_RSS12\"][int(length_2)+1:]\n",
    "        avg_RSS13 = dataframe[\"avg_RSS13\"][0:int(length_2)]\n",
    "        avg_RSS13_1 = dataframe[\"avg_RSS13\"][int(length_2)+1:]\n",
    "        var_RSS13 = dataframe[\"var_RSS13\"][0:int(length_2)]\n",
    "        var_RSS13_1 = dataframe[\"var_RSS13\"][int(length_2)+1:]\n",
    "        avg_RSS23 = dataframe[\"avg_RSS23\"][0:int(length_2)]\n",
    "        avg_RSS23_1 = dataframe[\"avg_RSS23\"][int(length_2)+1:]\n",
    "        var_RSS23 = dataframe[\"var_RSS23\"][0:int(length_2)]\n",
    "        var_RSS23_1 = dataframe[\"var_RSS23\"][int(length_2)+1:]\n",
    "\n",
    "\n",
    "    new_data[file] = pd.DataFrame({'avg_RSS12': list(avg_RSS12),'var_RSS12': list(var_RSS12),'avg_RSS13': list(avg_RSS13),'var_RSS13': list(var_RSS13),\n",
    "                                 'avg_RSS23': list(avg_RSS23),'var_RSS23': list(var_RSS23),'avg_RSS12_1': list(avg_RSS12_1),'var_RSS12_1': list(var_RSS12_1),\n",
    "                                 'avg_RSS13_1': list(avg_RSS13_1),'var_RSS13_1': list(var_RSS13_1),'avg_RSS23_1': list(avg_RSS23_1),'var_RSS23_1': list(var_RSS23_1)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances=[]\n",
    "ith_instance=[]\n",
    "df=list(new_data.values())\n",
    "for i in range(88):\n",
    "    \n",
    "    ith_instance=[min(df[i]['avg_RSS12']),\n",
    "    max(df[i]['avg_RSS12']),\n",
    "    stat.mean(df[i]['avg_RSS12']),\n",
    "    stat.median(df[i]['avg_RSS12']),\n",
    "    df[i]['avg_RSS12'].std(),\n",
    "    df[i]['avg_RSS12'].quantile(0.25),\n",
    "    df[i]['avg_RSS12'].quantile(0.75),\n",
    "    min(df[i]['var_RSS12']),\n",
    "    max(df[i]['var_RSS12']),\n",
    "    stat.mean(df[i]['var_RSS12']),\n",
    "    stat.median(df[i]['var_RSS12']),\n",
    "    df[i]['var_RSS12'].std(),\n",
    "    df[i]['var_RSS12'].quantile(0.25),\n",
    "    df[i]['var_RSS12'].quantile(0.75),\n",
    "    min(df[i]['avg_RSS13']),\n",
    "    max(df[i]['avg_RSS13']),\n",
    "    stat.mean(df[i]['avg_RSS13']),\n",
    "    stat.median(df[i]['avg_RSS13']),\n",
    "    df[i]['avg_RSS13'].std(),\n",
    "    df[i]['avg_RSS13'].quantile(0.25),\n",
    "    df[i]['avg_RSS13'].quantile(0.75),\n",
    "    min(df[i]['var_RSS13']),\n",
    "    max(df[i]['var_RSS13']),\n",
    "    stat.mean(df[i]['var_RSS13']),\n",
    "    stat.median(df[i]['var_RSS13']),\n",
    "    df[i]['var_RSS13'].std(),\n",
    "    df[i]['var_RSS13'].quantile(0.25),\n",
    "    df[i]['var_RSS13'].quantile(0.75),\n",
    "    min(df[i]['avg_RSS23']),\n",
    "    max(df[i]['avg_RSS23']),\n",
    "    stat.mean(df[i]['avg_RSS23']),\n",
    "    stat.median(df[i]['avg_RSS23']),\n",
    "    df[i]['avg_RSS23'].std(),\n",
    "    df[i]['avg_RSS23'].quantile(0.25),\n",
    "    df[i]['avg_RSS23'].quantile(0.75),\n",
    "    min(df[i]['var_RSS23']),\n",
    "    max(df[i]['var_RSS23']),\n",
    "    stat.mean(df[i]['var_RSS23']),\n",
    "    stat.median(df[i]['var_RSS23']),\n",
    "    df[i]['var_RSS23'].std(),\n",
    "    df[i]['var_RSS23'].quantile(0.25),\n",
    "    df[i]['var_RSS23'].quantile(0.75),\n",
    "    min(df[i]['avg_RSS12_1']),\n",
    "    max(df[i]['avg_RSS12_1']),\n",
    "    stat.mean(df[i]['avg_RSS12_1']),\n",
    "    stat.median(df[i]['avg_RSS12_1']),\n",
    "    df[i]['avg_RSS12_1'].std(),\n",
    "    df[i]['avg_RSS12_1'].quantile(0.25),\n",
    "    df[i]['avg_RSS12_1'].quantile(0.75),\n",
    "    min(df[i]['var_RSS12_1']),\n",
    "    max(df[i]['var_RSS12_1']),\n",
    "    stat.mean(df[i]['var_RSS12_1']),\n",
    "    stat.median(df[i]['var_RSS12_1']),\n",
    "    df[i]['var_RSS12_1'].std(),\n",
    "    df[i]['var_RSS12_1'].quantile(0.25),\n",
    "    df[i]['var_RSS12_1'].quantile(0.75),\n",
    "    min(df[i]['avg_RSS13_1']),\n",
    "    max(df[i]['avg_RSS13_1']),\n",
    "    stat.mean(df[i]['avg_RSS13_1']),\n",
    "    stat.median(df[i]['avg_RSS13_1']),\n",
    "    df[i]['avg_RSS13_1'].std(),\n",
    "    df[i]['avg_RSS13_1'].quantile(0.25),\n",
    "    df[i]['avg_RSS13_1'].quantile(0.75),\n",
    "    min(df[i]['var_RSS13_1']),\n",
    "    max(df[i]['var_RSS13_1']),\n",
    "    stat.mean(df[i]['var_RSS13_1']),\n",
    "    stat.median(df[i]['var_RSS13_1']),\n",
    "    df[i]['var_RSS13_1'].std(),\n",
    "    df[i]['var_RSS13_1'].quantile(0.25),\n",
    "    df[i]['var_RSS13_1'].quantile(0.75),\n",
    "    min(df[i]['avg_RSS23_1']),\n",
    "    max(df[i]['avg_RSS23_1']),\n",
    "    stat.mean(df[i]['avg_RSS23_1']),\n",
    "    stat.median(df[i]['avg_RSS23_1']),\n",
    "    df[i]['avg_RSS23_1'].std(),\n",
    "    df[i]['avg_RSS23_1'].quantile(0.25),\n",
    "    df[i]['avg_RSS23_1'].quantile(0.75),\n",
    "    min(df[i]['var_RSS23_1']),\n",
    "    max(df[i]['var_RSS23_1']),\n",
    "    stat.mean(df[i]['var_RSS23_1']),\n",
    "    stat.median(df[i]['var_RSS23_1']),\n",
    "    df[i]['var_RSS23_1'].std(),\n",
    "    df[i]['var_RSS23_1'].quantile(0.25),\n",
    "    df[i]['var_RSS23_1'].quantile(0.75)]\n",
    "    instances.append(ith_instance)\n",
    "\n",
    "df_instances=pd.DataFrame(instances,columns=['min1','max1','mean1','median1','standard deviation1','1st quart1','3rd quart1','min2','max2','mean2','median2','standard deviation2','1st quart2','3rd quart2','min3','max3','mean3','median3','standard deviation3','1st quart3','3rd quart3','min4','max4','mean4','median4','standard deviation4','1st quart4','3rd quart4','min5','max5','mean5','median5','standard deviation5','1st quart5','3rd quart5','min6','max6','mean6','median6','standard deviation6','1st quart6','3rd quart6','min7','max7','mean7','median7','standard deviation7','1st quart7','3rd quart7',\n",
    "                                            'min8','max8','mean8','median8','standard deviation8','1st quart8','3rd quart8',\n",
    "                                            'min9','max9','mean9','median9','standard deviation9','1st quart9','3rd quart9',\n",
    "                                            'min10','max10','mean10','median10','standard deviation10','1st quart10','3rd quart10',\n",
    "                                            'min11','max11','mean11','median11','standard deviation11','1st quart11','3rd quart11',\n",
    "                                            'min12','max12','mean12','median12','standard deviation12','1st quart12','3rd quart12'])\n",
    "\n",
    "df_instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the data with the required features\n",
    "df_KeptData2 = df_instances[[\"1st quart1\", \"median1\", \"3rd quart1\", \"1st quart2\", \"median2\", \"3rd quart2\", \"1st quart12\", \"median12\", \"3rd quart12\"]]\n",
    "df_KeptData2[\"bending\"] = [1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]  \n",
    "\n",
    "# 1 for bending, 0 for other activities\n",
    "df_KeptData2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_KeptData2, hue=\"bending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-conflict",
   "metadata": {},
   "source": [
    "no considerable change observed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-african",
   "metadata": {},
   "source": [
    "# 8-a(ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288228b7",
   "metadata": {},
   "source": [
    "The right way is to apply cross-validation in parallel with both selecting the significant predictors and applying the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the splits has been done by excel and has been saved in the data folder. now we load them:\n",
    "data_new={}\n",
    "data_new[1]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries1.csv')\n",
    "data_new[2]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries2.csv')\n",
    "data_new[3]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries3.csv')\n",
    "data_new[4]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries4.csv')\n",
    "data_new[5]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries5.csv')\n",
    "data_new[6]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries6.csv')\n",
    "data_new[7]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries7.csv')\n",
    "data_new[8]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries8.csv')\n",
    "data_new[9]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries9.csv')\n",
    "data_new[10]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries10.csv')\n",
    "data_new[11]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries11.csv')\n",
    "data_new[12]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries12.csv')\n",
    "data_new[13]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries13.csv')\n",
    "data_new[14]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries14.csv')\n",
    "data_new[15]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries15.csv')\n",
    "data_new[16]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries16.csv')\n",
    "data_new[17]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries17.csv')\n",
    "data_new[18]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries18.csv')\n",
    "data_new[19]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries19.csv')\n",
    "data_new[20]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseries20.csv')\n",
    "\n",
    "data_new[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc29879",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in data_new.items():\n",
    "    val.fillna(np.mean(val.iloc[43]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-invention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_new[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_score = {}\n",
    "best_features = {}\n",
    "for i in range(1,21):\n",
    "    X_train = data_new[i].drop([\"bending\"], axis=1)\n",
    "    y_train = data_new[i][\"bending\"]\n",
    "    fold=5\n",
    "    model= LogisticRegression(solver='liblinear')\n",
    "    rfe_cv = RFECV(estimator=model, cv=StratifiedKFold(5),scoring='accuracy')\n",
    "    rfe_cv.fit(X_train, y_train)\n",
    "    plt.plot(range(1, len(rfe_cv.grid_scores_) + 1), rfe_cv.grid_scores_)\n",
    "    print('l=',i)\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"validation score\")\n",
    "    plt.show()\n",
    "    col_names = []\n",
    "    for index, col in enumerate(X_train.columns):\n",
    "        if rfe_cv.support_[index] == True:\n",
    "            col_names.append(col) \n",
    "    best_features[i] = col_names\n",
    "    reg_score[i] = rfe_cv.grid_scores_[rfe_cv.n_features_ - 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_score_data=pd.DataFrame.from_dict(reg_score,orient='index')\n",
    "reg_score_data.rename(columns={0:'scores'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "max_score_index=max(reg_score.items(), key=operator.itemgetter(1))[0]\n",
    "max_score_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_new[max_score_index][[\"bending\"]]\n",
    "best_features[max_score_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103602d0",
   "metadata": {},
   "source": [
    "# a(iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_columns = best_features[max_score_index]\n",
    "x_train = data_new[max_score_index][best_columns]\n",
    "y_train = y_train\n",
    "\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(x_train, y_train)\n",
    "prediction = model.predict(x_train)\n",
    "\n",
    "fp, tp, th = roc_curve(y_train, prediction)\n",
    "roc_auc = auc(fp, tp)\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(15, 6), dpi=80)\n",
    "plt.plot(fp, tp,'r--', label='ROC curve (roc_auc_area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'b--')\n",
    "\n",
    "plt.xlabel('False Positive Rate(FPR)')\n",
    "plt.ylabel('True Positive Rate(TPR)')\n",
    "plt.title('False Positive Rate(FPR) vs True Positive Rate(TPR): ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.02])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_params = sm.Logit(y_train, x_train).fit_regularized()\n",
    "print(log_reg_params.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-candidate",
   "metadata": {},
   "source": [
    "# a(iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the splits has been done by excel and has been saved in the data folder. now we load them:\n",
    "data_new_test={}\n",
    "data_new_test[1]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest1.csv')\n",
    "data_new_test[2]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest2.csv')\n",
    "data_new_test[3]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest3.csv')\n",
    "data_new_test[4]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest4.csv')\n",
    "data_new_test[5]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest5.csv')\n",
    "data_new_test[6]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest6.csv')\n",
    "data_new_test[7]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest7.csv')\n",
    "data_new_test[8]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest8.csv')\n",
    "data_new_test[9]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest9.csv')\n",
    "data_new_test[10]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest10.csv')\n",
    "data_new_test[11]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest11.csv')\n",
    "data_new_test[12]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest12.csv')\n",
    "data_new_test[13]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest13.csv')\n",
    "data_new_test[14]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest14.csv')\n",
    "data_new_test[15]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest15.csv')\n",
    "data_new_test[16]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest16.csv')\n",
    "data_new_test[17]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest17.csv')\n",
    "data_new_test[18]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest18.csv')\n",
    "data_new_test[19]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest19.csv')\n",
    "data_new_test[20]=pd.read_csv(r'C:\\Users\\Black Tornado0\\Desktop\\fiv\\dataHW4\\timeseriestest20.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in data_new_test.items():\n",
    "    val.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_test[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = data_new_test[max_score_index][[\"bending\"]]\n",
    "x_test = data_new_test[max_score_index][best_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d8114",
   "metadata": {},
   "source": [
    "the accuracy on the test set is 94%, while the cross validation accuracy was 87%. there is no cosiderable difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-compound",
   "metadata": {},
   "source": [
    "# a(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d07d05",
   "metadata": {},
   "source": [
    "the data is well seperated and it causes instability in the calculation of parameters of Logistic Regression.\n",
    "This causes high p-values of the estimates, or this causes MLE estimate fail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b150c59",
   "metadata": {},
   "source": [
    "# a(vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-latter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_control = x_train\n",
    "data_control['bending'] = [1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] \n",
    "data_control_majority = data_control.loc[data_control['bending']==0]\n",
    "data_control_minority = data_control.loc[data_control['bending']==1]\n",
    "samples=64\n",
    "data_control_minority_upsample = resample(data_control_minority,n_samples=samples,replace=True,random_state=42)\n",
    "data_control_upsample = pd.concat([data_control_majority, data_control_minority_upsample])\n",
    "\n",
    "\n",
    "print(data_control_upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = data_control_upsample['bending']\n",
    "input_cols = data_control_upsample.drop('bending', axis=1)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(input_cols, target_col)\n",
    "prediction = model.predict(input_cols)\n",
    "fp, tp, th = roc_curve(target_col, prediction)\n",
    "roc_auc = auc(fp, tp)\n",
    "\n",
    "figure(figsize=(15, 6), dpi=80)\n",
    "plt.plot(fp, tp,'r--', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'b--')\n",
    "\n",
    "plt.xlabel('False Positive Rate(FPR)')\n",
    "plt.ylabel('True Positive Rate(TPR)')\n",
    "plt.title('False Positive Rate(FPR) vs True Positive Rate(TPR): ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.02])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(target_col, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-associate",
   "metadata": {},
   "source": [
    "# (b)(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L1_penalized_reg():\n",
    "        log_score_l1 = []\n",
    "        i=1\n",
    "        while i < 21:\n",
    "            xtrain = data_new[i].drop([\"bending\"], axis=1)\n",
    "            ytrain = data_new[i][\"bending\"]\n",
    "            model = LogisticRegressionCV(cv=5, penalty=\"l1\", solver=\"liblinear\")\n",
    "            model.fit(xtrain, ytrain)\n",
    "            score = model.score(xtrain, ytrain) \n",
    "            log_score_l1.append(score)\n",
    "            i=i+1\n",
    "        return log_score_l1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_score_l1=L1_penalized_reg()\n",
    "log_score_l1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa1958",
   "metadata": {},
   "source": [
    "# (b)(ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6256e2cf",
   "metadata": {},
   "source": [
    "We can see the accuracy scores for L1-penalized is larger than for all values of l, so, we can say it is performing better. It was also easier to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddaabd0",
   "metadata": {},
   "source": [
    "# c(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49aa735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial():\n",
    "    log_reg_error = []\n",
    "    log_reg_train_score = []\n",
    "    i=1\n",
    "    while i < 21:\n",
    "        xtrain = data_new[i].drop([\"bending\"], axis=1)\n",
    "        ytrain = data_new[i][[\"bending\"]]\n",
    "        ytrain[\"result_column\"]= [1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6]\n",
    "        ytrain = ytrain.result_column\n",
    "        xtest = data_new_test[i].drop([\"bending\"], axis=1)\n",
    "        ytest = data_new_test[i][[\"bending\"]]\n",
    "\n",
    "        ytest[\"result_column\"]=[1,1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6]\n",
    "        ytest = ytest.result_column\n",
    "\n",
    "        model = LogisticRegressionCV(cv=5, penalty=\"l1\", solver=\"liblinear\")\n",
    "        model.fit(xtrain, ytrain)\n",
    "        test_error = 1 - model.score(xtest, ytest) \n",
    "        log_reg_train_score.append(model.score(xtrain, ytrain))\n",
    "        log_reg_error.append(test_error)\n",
    "        i=i+1\n",
    "    print(log_reg_error)\n",
    "    return log_reg_error.index(min(log_reg_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20319ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_index=multinomial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_multiclass():\n",
    "        xtrain = data_new[optimized_index].drop([\"bending\"], axis=1)\n",
    "        ytrain = data_new[optimized_index][[\"bending\"]]\n",
    "        ytrain[\"result_column\"]= [1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6]\n",
    "        ytrain = ytrain.result_column\n",
    "        xtest = data_new_test[optimized_index].drop([\"bending\"], axis=1)\n",
    "        ytest = data_new_test[optimized_index][[\"bending\"]]\n",
    "        ytest[\"result_column\"]=[1,1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6]\n",
    "        ytest = ytest.result_column\n",
    "\n",
    "        model = LogisticRegression(solver=\"liblinear\")\n",
    "        model.fit(xtrain, ytrain)\n",
    "\n",
    "        prediction = model.predict(xtest)\n",
    "\n",
    "        return confusion_matrix(ytest, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=confusion_multiclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = data_new[optimized_index].drop([\"bending\"], axis=1)\n",
    "ytrain = data_new[optimized_index][[\"bending\"]]\n",
    "ytrain[\"result_column\"]= [1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6]\n",
    "ytrain = ytrain.result_column\n",
    "multi_class_train=label_binarize(ytrain,classes=[1,2,3,4,5,6])\n",
    "classes = multi_class_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec13a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = data_new_test[optimized_index].drop([\"bending\"], axis=1)\n",
    "ytest = data_new_test[optimized_index][[\"bending\"]]\n",
    "ytest[\"result_column\"]=[1,1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6]\n",
    "ytest = ytest.result_column\n",
    "multi_class_test=label_binarize(ytest,classes=[1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cv=OneVsRestClassifier(LogisticRegressionCV(cv=StratifiedKFold(5),penalty=\"l1\",solver=\"liblinear\"))\n",
    "log_reg_cv.fit(xtrain,multi_class_train)\n",
    "prediction=log_reg_cv.predict(xtest)\n",
    "log_reg_cv_score=log_reg_cv.fit(xtrain,multi_class_train).decision_function(xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b813732",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = dict()\n",
    "tp = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(classes):\n",
    "    fp[i], tp[i],threshold= roc_curve(multi_class_test[:, i],log_reg_cv_score[:, i])\n",
    "    roc_auc[i] = auc(fp[i], tp[i])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "plt.plot(fp[0],tp[0],label='class 0(%0.2f)'%roc_auc[0])\n",
    "plt.plot(fp[1],tp[1],label='class 1(%0.2f)'%roc_auc[1])\n",
    "plt.plot(fp[2],tp[2],label='class 2(%0.2f)'%roc_auc[2])\n",
    "plt.plot(fp[3],tp[3],label='class 3(%0.2f)'%roc_auc[3])\n",
    "plt.plot(fp[4],tp[4],label='class 4(%0.2f)'%roc_auc[4])\n",
    "plt.plot(fp[5],tp[5],label='class 5(%0.2f)'%roc_auc[5])\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0f45a",
   "metadata": {},
   "source": [
    "# c(ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1a72d",
   "metadata": {},
   "source": [
    "Naive Bayes assumes conditional independence of the features. However features extracted from the time series are correlated, and hence the Naive assumption is not strong and the classifier might not perform well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Naive Bayes\n",
    "gauss_score = []\n",
    "while i < 21:\n",
    "        xtrain = data_new[i].drop([\"bending\"], axis=1)\n",
    "        ytrain = data_new[i][[\"bending\"]]\n",
    "        ytrain[\"result_column\"]= [1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6]\n",
    "        ytrain = ytrain.result_column\n",
    "\n",
    "        model = GaussianNB()\n",
    "        scores = cross_val_score(model, xtrain, ytrain, cv=5)\n",
    "        gauss_score.append(np.mean(scores))  \n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe252c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37666c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_scored_series=gauss_score.index(max(gauss_score))\n",
    "best_columns = best_features[optimal_scored_series]\n",
    "xtrain = data_new[optimal_scored_series][best_columns]\n",
    "ytrain = data_new[optimal_scored_series][[\"bending\"]]\n",
    "ytrain[\"result_column\"]= [1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6]\n",
    "ytrain = ytrain.result_column\n",
    "xtest = data_new_test[optimal_scored_series][best_columns]\n",
    "ytest = data_new_test[optimal_scored_series][[\"bending\"]]\n",
    "ytest[\"result_column\"]=[1,1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6]\n",
    "ytest = ytest.result_column\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "prediction_test=model.predict(xtest)\n",
    "print(confusion_matrix(ytest, prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "MNB_score = []\n",
    "i=1\n",
    "while i < 21:\n",
    "        xtrain = data_new[i].drop([\"bending\"], axis=1)\n",
    "        ytrain = data_new[i][[\"bending\"]]\n",
    "        ytrain[\"result_column\"]= [1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6]\n",
    "        ytrain = ytrain.result_column\n",
    "\n",
    "        model = MultinomialNB()\n",
    "        scores = cross_val_score(model, xtrain, ytrain, cv=5)\n",
    "        MNB_score.append(np.mean(scores))\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d949b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MNB_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de6242",
   "metadata": {},
   "source": [
    "as observed, the multinomial Naive Bayes classifier has higher accuracy than the Guassian Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_scored_series=MNB_score.index(max(MNB_score))\n",
    "\n",
    "best_columns = best_features[optimal_scored_series]\n",
    "xtrain = data_new[optimal_scored_series][best_columns]\n",
    "ytrain = data_new[optimal_scored_series][[\"bending\"]]\n",
    "ytrain[\"result_column\"]= [1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6]\n",
    "ytrain = ytrain.result_column\n",
    "xtest = data_new_test[optimal_scored_series][best_columns]\n",
    "ytest = data_new_test[optimal_scored_series][[\"bending\"]]\n",
    "ytest[\"result_column\"]=[1,1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6]\n",
    "ytest = ytest.result_column\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "prediction_test=model.predict(xtest)\n",
    "print(confusion_matrix(ytest, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dc807",
   "metadata": {},
   "source": [
    "# c(iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9846de",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = data_new[optimized_index].drop([\"bending\"], axis=1)\n",
    "ytrain = data_new[optimized_index][[\"bending\"]]\n",
    "ytrain[\"result_column\"]= [1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6]\n",
    "ytrain = ytrain.result_column\n",
    "multi_class_train=label_binarize(ytrain,classes=[0,1,2,3,4,5])\n",
    "classes = multi_class_train.shape[1]\n",
    "\n",
    "xtest = data_new_test[optimized_index].drop([\"bending\"], axis=1)\n",
    "ytest = data_new_test[optimized_index][[\"bending\"]]\n",
    "ytest[\"result_column\"]=[1,1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6]\n",
    "ytest = ytest.result_column\n",
    "multi_class_test=label_binarize(ytest,classes=[1,2,3,4,5,6])\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    " \n",
    "X_train = pca.fit_transform(xtrain)\n",
    "X_test = pca.transform(xtest)\n",
    " \n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_gauss_score=[]\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, ytrain)\n",
    "prediction_train=model.predict(X_train)\n",
    "scores = cross_val_score(model, X_train, ytrain, cv=5)\n",
    "#cross val score\n",
    "pca_gauss_score.append(np.mean(scores))\n",
    "pca_gauss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score\n",
    "prediction_test=model.predict(X_test)\n",
    "accuracy_score(ytest,prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ffefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_comp_train_data = pd.DataFrame(data = X_train, columns = ['principal_component_1', 'principal_component_2'])\n",
    "principal_comp_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f98d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fda9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train=pd.DataFrame(data=prediction_train,columns=['class value'])\n",
    "prediction_train.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(ytrain, prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(X_train)\n",
    "score=model.fit(X_train, ytrain).predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd9a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_rate=dict()\n",
    "tp_rate=dict()\n",
    "r_auc=dict()\n",
    "classes=multi_class_train.shape[1]\n",
    "for i in range(classes):\n",
    "    fp_rate[i], tp_rate[i], thresholds = roc_curve(ytrain, score[:,i],pos_label=i)\n",
    "    r_auc[i]=auc(fp_rate[i], tp_rate[i])\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(fp_rate[1], tp_rate[1],label='class2(%2f)'%r_auc[1])\n",
    "plt.plot(fp_rate[2], tp_rate[2],label='class3(%2f)'%r_auc[2])\n",
    "plt.plot(fp_rate[3], tp_rate[3],label='class4(%2f)'%r_auc[3])\n",
    "plt.plot(fp_rate[4], tp_rate[4],label='class5(%2f)'%r_auc[4])\n",
    "plt.plot(fp_rate[5], tp_rate[5],label='class6(%2f)'%r_auc[5])\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c6b43",
   "metadata": {},
   "source": [
    "# c(iv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f589aa5",
   "metadata": {},
   "source": [
    "the Multinomial Naive Bayes classifier has the highest accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
